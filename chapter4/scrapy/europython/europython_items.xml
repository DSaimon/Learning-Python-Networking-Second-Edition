<?xml version="1.0" encoding="utf-8"?>
<items><item><title><value>Citizen Science with Python</value></title><author><value>Ian Ozsvald</value></author><description><value>You could make a difference in the world with a little science and Python. We’ll look at several data-driven humanitarian and healthcare projects developed using Python and, all going well, run some audience experiments. By the end of the talk I hope you’ll be looking to run your own experiments with the scientific Python stack.</value></description><date><value>Friday 27 July</value></date><tags><value>Science</value><value>Data</value></tags></item><item><title><value>How do I get the job I want? </value></title><author><value>Franziska Schropp</value></author><description><value>Engineers are rare, jobs are plenty, so far so good. But what if I don’t just want any job that pays the bills, but the job that will push my career and that I actually enjoy doing every day? If your answer to why you’d like to join company XY is: “Work from home and make good money”, you should not be surprised to not be taken into the next round. But what would be a good answer to that question? What should I say when asked where I see myself in 5 years? There are a lot of subtle aspects to succeeding and failing in the application process, from your initial cover letter to the on site visit meeting the team.  As a tech recruiter, I’d like to share with you the best and (sometimes quite entertaining) worst practices in the application process in order to help you get the job you want! </value></description><date><value>Wednesday 25 July</value></date><tags><value>The Answer to Life the Universe and Everything Else</value><value>workforce</value><value>Community</value></tags></item><item><title><value>How is python used in biomolecular sciences?</value></title><author><value>Antonia Mey</value></author><description><value>In the last ten years scientists working on computational problems involving proteins and other small molecules have largely moved to using python when developing simulation and data analysis tools allowing for a fast prototyping and development of new ideas. One big challenge is dealing with the compatibility of different tools and using these to create very complex adaptive, yet robust workflows in order to be able to guide cutting edge experiments e.g. predicting how well a small drug like molecule can bind to a protein that could serve as a target for a new drug. The talk will give a gentle introduction to what kind of python related tools are available in the field  of computational molecular biology, how they are used, and what kind of complex workflows scientist have to solve.
I will then introduce BioSimSpace an open source python library and flagship project of the CCPBioSim consortium in the UK, which provides a common API to avoid having to learn many individual tools facing compatibility and dependency challenges allowing scientists to focus on the scientific question at hand and not solving programming challenges. BioSimSpace allows fast and interoperable building of workflow components (nodes) for bimolecular problems, which can easily be used on a variety of different computational resources. In particular I will introduce the cloud facilities available for fast prototyping using a Jupyter notebook interface. </value></description><date><value>Wednesday 25 July</value></date><tags><value>Physics</value><value>Visualization</value><value>Data</value><value>Jupyter/iPython</value><value>Scientific Libraries (Numpy/Pandas/SciKit/...)</value></tags></item><item><title><value>Hello to the World in 8 Web Frameworks (micro, batteries included &amp; async)</value></title><author><value>Aaron Bassett</value></author><description><value>Join us for a Phileas Fogg inspired whirlwind tour of eight Python web frameworks.</value><value>“Had he programmed? It was likely, for no one seemed to know web frameworks more familiarly; there was no framework so obscure that he did not appear to have an intimate acquaintance with it. He must have developed in everything, at least in the spirit.”</value><value>We’ll start with the current crop of microframeworks, showing how to achieve the same task in each, before progressing to “Batteries included” and the more specialised async frameworks.</value><value>For developers who perhaps have only used a single framework, this talk gives them an opportunity to get out and explore the world (of web frameworks) and broaden their horizons, with plenty of Jules Verne inspired fun along the way.</value></description><date><value>Thursday 26 July</value></date><tags><value>Fun and Humor</value><value>Pyramid</value><value>ASYNC / Concurreny</value><value>Internet</value><value>Django</value></tags></item><item><title><value>How async and await ended up in Python</value></title><author><value>Pavlin Gergov</value></author><description><value>We’re going to talk about regular functions,  iterables, iterators, function execution and yield - generator functions. We will send values and play with generators a bit. A quick look at asyncio will be followed by async and await, what a coroutine is and how to write async code with Python.</value><value>Code snippets can be found here: </value><value>https://github.com/pgergov/europython-2018</value></description><date><value>Friday 27 July</value></date><tags><value>Beginners</value><value>Education</value><value>Python 3</value><value>ASYNC / Concurreny</value></tags></item><item><title><value>How can you use Open Source materials to learn Python &amp; data science?</value></title><author><value>Kamila Stepniowska</value></author><description><value>Python is very often recommended as the language of choice in a programming education. I can see at least two cases when it’s a thing:
- introduction to programming - regardless an age and any previous educational experience,
- data science - it’s just a standard…
and actually both - you can teach a future data scientists starting by teaching Python. </value><value>During this talk, I would like to briefly present you what Open Source Python educational materials do we have there and how those are and can be used to teach Python and data science. PyLadies, Django Girls, Django Carrots, Python Software Foundation creates many very available materials. From the data science side, you have Open Source materials created by kaggle, Minerva, Github repos, and many other organizations and individuals. </value><value>During this talk you will learn:
- where to find an Open Source Python and data science tutorials and educational materials
- how does the Python community can support your learning process
- why learning data science with Python is a good idea.</value><value>I’m also interested in learning your educational experience with an Open Source materials and Python community supporting your learning experience. If you would like to share a link or your educational story,  please feel free to send me an email on kamila@stepniowski.com. If I will find it interesting for the audience and if you will give me your permission to share it, I might use it in the talk.</value></description><date><value>Thursday 26 July</value></date><tags><value>Education</value><value>Diversity</value><value>Open-Source</value><value>Community</value><value>Learning</value></tags></item><item><title><value>Hacking Reinforcement Learning</value></title><author><value>Guillem Duran</value></author><description><value>Repository</value><value>Slides with notes</value><value>Creating huge datasets of top performing examples for Reinforcement Learning (RL) has always been tricky, but if we allow
ourselves to cheat a bit it can be done very easily. During this talk, I will present a new family of algorithms that allow to efficiently generate very high quality samples for any known RL environment.</value><value>This new generation of planning algorithms achieves a performance which is several orders of magnitude higher
than any other existing alternative, while offering linear time complexity and good scalability.</value><value>This talk will be a practical example
of how we can use new tools for hacking any reinforcement learning environment, and make it generate superhuman level games.</value><value>Hacking RL, as any other hacking process will be divided in four phases:</value><value>During information gathering, I will briefly explain what are the main ideas behind Reinforcement Learning.
I will also talk about how our theory (FractalAI) came to be, and what are the fundamental concepts behind it.</value><value>We will find an attack vector against the environment API, and explain how it can be exploited. I will explain
the fundamental concepts needed to build a new generation of exploits, that will allow us to have complete control over the data the environment produces.</value><value>This is the time to test the new exploits and to show a proof of concept. We will exploit the attack vector to gain access
to the environment. Using only a laptop I will show how it is possible to sample data which surpasses human performance way faster than real time.</value><value>Once we have gained control of the environment, we will measure how well the exploits work, and how well the techniques presented
can generalize to other types of environments.</value><value>I want the talk to be as simple and fast as possible, with a lot of graphical examples, videos, and a Jupyter notebook.
The Q&amp;A session is the time to apply some social engineering to get me to talk about the details that you find more interesting.
I have prepared additional material covering the most common questions and concerns, but feel free to ask whatever you want, I love challenging questions ;)</value><value>Some of the topics covered in the additional material are:</value><value>Google spreadsheet with all bencharks on Atari</value><value>Code used to run the examples</value><value>. (Not merged to the FractalAI repo yet)</value></description><date><value>Wednesday 25 July</value></date><tags><value>Science</value><value>Case Study</value><value>Algorithms</value><value>The Answer to Life the Universe and Everything Else</value><value>Deep Learning</value></tags></item><item><title><value>Good features beat algorithms</value></title><author><value>Pietro Mascolo</value></author><description><value>In Machine Learning and Data Science in general, understanding the data is paramount. This understanding can come from many different sources and techniques: domain expertise, exploratory analysis, SMEs, some specific Machine Learning techniques, and feature engineering.
As a matter of fact, most Machine Learning and Statistical analysis strongly depends on how the data is prepared, thus making feature engineering very important for any serious Machine Learning enterprise.</value><value>“Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.”</value><value>In this talk we will discuss what feature engineering  and feature selection are; how to select important features in a real-world dataset and how to develop a simple, but powerful ensemble to measure feature importance and perform feature selection.</value><value>Familiarity with intermediate concepts of the Python programming language is required to follow the implementation steps. General knowledge of the basic concepts of Machine Learning and data cleaning will be useful, but not strictly necessary, to follow the discussion on feature selection and feature engineering.</value></description><date><value>Friday 27 July</value></date><tags><value>Algorithms</value><value>Data Science</value><value>Data</value><value>Python 3</value><value>Machine-Learning</value></tags></item><item><title><value>Getting Started with Mypy and Type Checking</value></title><author><value>Jukka Lehtosalo</value></author><description><value>Ever wondered if you should try type checking in your Python project, or what it takes to get started with mypy? I will show how to introduce type checking the easy way, one step at a time.</value><value>Dropbox was an early adopter of type checking and mypy, and we’ve been gradually adding type annotations to our large production codebases since 2016. Engineers love how type annotations improve the clarity of code, and how mypy can surface hard-to-find bugs. We’ve learned a lot in the process of annotating millions of lines of code, and in this talk you’ll learn battle-tested approaches for adding type checking to an existing codebase.</value><value>I expect no previous experience with mypy. You’ll benefit the most if you’ve worked on a project with multiple developers.</value></description><date><value>Thursday 26 July</value></date><tags><value>Best Practice</value><value>Type-Hinting</value><value>Tooling</value><value>Code Analysis</value></tags></item><item><title><value>Get Productive with Python in Visual Studio Code</value></title><author><value>Dan Taylor</value></author><description><value>In this demo driven session, we’ll show you how to use the cross-platform, free, and open source Visual Studio Code for all your Python development needs. From editing, to linting, to debugging and more, you will learn how to get started, as well as tips and tricks to save you time in your everyday development lifecycle.</value><value>If you’re an experienced Python developer, you will learn how to take advantage of all of Visual Studio Code’s capabilities to maximize your productivity. If you’re a new Python developer, you will learn more about Python and how to use Visual Studio Code to get up and running quickly.</value><value>We will start by showing how to create a new application, configure linting, manage virtual environments, and run code. Then we’ll show how to use more powerful features like debugging, unit testing, the Docker extension, and Visual Studio Live Share for collaborative editing and debugging with your co-workers. </value><value>Code available at: http://github.com/qubitron/pydemo</value></description><date><value>Friday 27 July</value></date><tags><value>Development</value><value>Debugging</value><value>Programming</value><value>Tooling</value><value>Python general</value></tags></item><item><title><value>Fuzzy Matching - Smart Way of Finding Similar Names Using Fuzzywuzzy</value></title><author><value>Cheuk Ting Ho</value></author><description><value>Matching strings should be one of the first natural language processing problem that human encounter since we start use computer to handle data. Unlike numerical value which has an exact logic to compare them, it is very hard to say how alike two strings are for a computer. One may compare them character by character and have an idea of how many characters in the pair of stings are the same. Unfortunately in most application we need computer to perceive strings like we do and therefore we have to use fuzzy matching. Fuzzy matching on names is never straight forward though, the definition of how “difference” of two names are really depends case by case. For example with restaurant names, matching of words like “cafe” “bar” and “restaurant” are consider less valuable then matching of some other less common words. Also, do we consider company names that matches partly (like “Happy Unicorn company” and Happy Unicorn co.”) are the same?</value><value>In the first half of the talk Levenshtein Distance, a measure of the similarity between two strings, will be explained. Different functions in Fuzzywuzzy like “partial</value><value>ratio” and “token</value><value>sort_ratio” will also be explored and compared for difference. It is very important to understand our tool and choose the right one for our task. Then in the second half, we will start tackling the example problem: matching company names, we will show that besides using Fuzzywuzzy, we have to also handle problem like finding and avoid matching of common words and speeding up the matching process by grouping the names. By combining all tricks and techniques that we demonstrate, we will also evaluate how efficient this method is and the advantage of using this method.</value><value>This talk is for people in all level of Python experience who would like to learn a trick or two and would like to be able to solve similar problems in the future. Theory of how the library works will be explained and It is easy to be pick up even for beginners.</value></description><date><value>Wednesday 25 July</value></date><tags><value>Beginners</value><value>Case Study</value><value>Data</value><value>Open-Source</value><value>Natural Language Processing</value></tags></item><item><title><value>From Zero to Azure with Python, Docker containers, and Visual Studio Code</value></title><author><value>Dan Taylor</value></author><description><value>In this session we will walk through creating a Python web app
with Docker and deploying it to Microsoft Azure using the free,
cross-platform, and open-source Visual Studio Code. We will also show
using hosted Jupyter notebooks in Azure Notebooks to analyze data, and
storing data in CosmosDB using the Azure SDK for Python. If you’re
excited about Python and want to learn more about the capabilities of
Azure, then this session is for you. We will cover all of the basics, no
experience with Azure, containers, cloud computing, or Visual Studio
Code necessary!</value><value>Code Repository: https://github.com/qubitron/zerotoazure-flaskcosmos</value></description><date><value>Wednesday 25 July</value></date><tags><value>Windows</value><value>Web</value><value>Docker</value><value>Cross-Platform-Development</value></tags></item><item><title><value>From linear algebra to machine learning</value></title><author><value>Omar Gutiérrez</value></author><description><value>Math is a crucial skill for people who are interested in Data Science and Machine Learning. Until now, most of the people who are doing Data Science have a strong background in math, usually, people with master or Ph.D. degrees. </value><value>However, this fact seems to change in the next years, after the hype of Machine Learning we are facing a process of </value><value>democratization</value><value>. Now the door of Data Science is open for everyone.</value><value>To </value><value>truly madly deeply</value><value> understand how the machine learning algorithms work we need to understand some mathematical concepts. In this tutorial, I would like to share my experience in the process of learning some of those concepts. </value><value>What I want to do is build a bridge between those concepts and Python, more specifically, </value><value>SciPy</value><value> and </value><value>NumPy</value><value> and </value><value>TensorFlow</value><value>.  Basically is </value><value>just another tutorial about vectorization</value><value>, in this case, oriented to understand and implement machine learning algorithms and the mathematical foundation that supports it.</value><value>The material of the talk can be found </value><value>here</value></description><date><value>Friday 27 July</value></date><tags><value>Scientific Libraries (Numpy/Pandas/SciKit/...)</value><value>Data Science</value><value>Data</value><value>Jupyter/iPython</value><value>Machine-Learning</value></tags></item><item><title><value>Faster Python startup</value></title><author><value>Jeethu Rao</value></author><description><value>The cPython’s startup speed generally quite fast compared to other similar dynamic language interpreters. Despite this, the interpreter’s startup time increases linearly with the number and size of imported python modules. Although interpreter startup speed isn’t a significant concern in long running servers, it does matter for the command line and other frequently launched applications.</value><value>One of the oldest tricks in the book, when it comes to performance optimizations is to perform frequent and expensive operations fewer times and reuse results from previous invocations. We noticed that the overhead of reading and un-marshalling .pyc files at startup can be eliminated if we could directly embed code objects and their associated object graph from .pyc files into the data segment of the cPython binary. This technique is quite similar to the approach taken by image based languages like Smalltalk in the past. Implementing this for cPython is made simpler because marshaled code objects in .pyc files contain a subset of the types of objects that marshal format supports. With this approach, loading a module included in the python binary is as cheap as dereferencing a pointer, albeit at the cost of increased binary size.</value><value>This talk will discuss the approach taken to implement the aforementioned idea for Python 3.6 and the challenges faced in implementing it. It will also talk about benchmark results from the improvements and possible future directions for this work. Although this talk delves into cPython internals, no prior experience with cPython internals is required to follow along.</value></description><date><value>Friday 27 July</value></date><tags><value>Performance</value><value>Compiler and Interpreters</value><value>CPython</value></tags></item><item><title><value>ETL pipeline to achieve reliability at scale</value></title><author><value>Isabel Lopez</value></author><description><value>In an online betting exchange, thousands of money related transactions are generated per minute. This data flow transforms a common and, in general, tedious task such as accounting into an interesting big data engineering problem. At Smarkets, accounting reports serve two main purposes: housekeeping of our financial operations and documentation for the relevant regulation authorities. In both cases, reliability and accuracy are crucial in the final result. The fact that these reports are generated daily, the need to cope with failure when retrieving data from previous days, and the fast growing transaction volume obsoleted the original accounting system and required a new pipeline that could scale.</value><value>This talk presents the ETL pipeline designed to meet the constraints highlighted above, and explains the motivations behind the tech stack chosen for the job, which includes Python3, Luigi and Spark among others. These topics will be covered by describing the main technical problems solved with our design:
- Fault tolerance and reliability, i.e ability to identify faulty steps and only rerun those instead of the whole pipeline.
- Fast input/output.
- Fast computations.</value></description><date><value>Wednesday 25 July</value></date><tags><value>Case Study</value><value>Python 3</value><value>Big Data</value><value>Scientific Libraries (Numpy/Pandas/SciKit/...)</value></tags></item><item><title><value>The pytest/tox/devpi help desk</value></title><author><value>Oliver Bestwalter</value></author><description><value>We’ll try to help everyone with their questions around pytest, tox and devpi and how they work together to test and release your packages.</value><value>If you have concrete questions or just want to learn more about these tools, come around and have a chat with us.</value><value>Potential topics and questions are:</value></description><date><value>Wednesday 25 July</value></date><tags><value>Best Practice</value><value>Packaging</value><value>Testing</value></tags></item><item><title><value>Exploring the Python AST Ecosystem</value></title><author><value>Chase Stevens</value></author><description><value>Materials are available at https://github.com/hchasestevens/europython-2018</value><value>This session will introduce attendees to Python’s rich ecosystem of abstract syntax tree tooling and libraries, with an emphasis on practical applications in static analysis and metaprogramming. Attendees should be fully comfortable with Python syntax and semantics, but familiarity with the ast module itself will not be necessary. </value><value>The talk will begin with a conceptual overview of ASTs, including a brief look at Python’s built-in introspection capabilities. It will introduce tools for AST visualization (astor, showast, python-ast-explorer), creation (asttools, meta), and transformation to source code (codegen). </value><value>How the AST can be used for static analysis will be covered; this will include discussion of Python’s built-in facilities (NodeVisitor) as well as of the 3rd party tools astsearch, astpath, and bellybutton. The talk will demonstrate the advantages and limits of these tools in comparison to other static analysis tooling (pylint, mypy); particular attention will be paid to how these tools can be incorporated into attendees’ workflows and  existing codebases and projects. </value><value>Tooling for Python AST manipulation and metaprogramming will be the final topic covered, focusing on the use of the NodeTransformer built-in. The talk will cover practical applications and examples of metaprogramming, such as metaprogramming for DSLS (pony, xpyth), runtime code manipulation (patterns, yield-from), and others (e.g. assertion rewriting in pytest).</value><value>While the talk will touch only briefly on each of the applications discussed, by the end of the session attendees should have a firm grasp of the kinds of problems the AST can be used to solve, what existing AST tooling can accomplish, and what resources are available for the development of their own AST tools.</value></description><date><value>Friday 27 July</value></date><tags><value>Use Case</value><value>Python general</value><value>Tooling</value><value>Code Analysis</value><value>Static Analysis</value></tags></item><item><title><value>OpenStack Help Desk</value></title><author><value>Daniel Abad</value></author><description><value>Come and chat with us about OpenStack! The free and open-source software platform for cloud computing. Get the advantages of infrastructure-as-a-service without the vendor lockin.</value><value>We can answer questions about OpenStack, help you with specific problems or help you get started with contributing to an OpenStack project. Your helpers will be OpenStack developers with years of experience using and developing on a number ofdifferent OpenStack projects.</value><value>We will  have a OpenStack deployment that we can use to demonstrate some of the possibilities.</value></description><date><value>Friday 27 July</value></date><tags><value>OpenStack</value></tags></item><item><title><value>Help desk: choosing (or not) the right NoSQL database</value></title><author><value>Alexys Jacob</value></author><description><value>During this </value><value>help desk</value><value>, some Numberly’s folks will make their </value><value>field expertise</value><value> available to </value><value>help you decide on the right database</value><value> for your project.</value><value>The NoSQL databases ecosystem evolved a lot over the last decade and it may be hard to keep up with all their strengths and design patterns. So let’s take this occasion to discuss about them!</value><value>If you are </value><value>starting up a new project or facing problems and limitations with your current implementation</value><value>, we will try our best to help you choose (or not) the right NoSQL database.</value></description><date><value>Wednesday 25 July</value></date><tags><value>Infrastructure</value><value>System Architecture</value><value>Distributed Systems</value><value>NoSQL</value><value>Databases</value></tags></item><item><title><value>Extending Python with C/C++</value></title><author><value>Paul Ross</value></author><description><value>If you want your Python code to run really fast you can get dramatic performance improvements if you are willing to write some of your code in C/C++. There are several ways to do this from traditional Python C extensions to using environments such as Cython or Pybind11. This help desk is here so you can choose the best approach to achieve that magical 100x performance increase without pain.</value></description><date><value>Friday 27 July</value></date><tags><value>Cython</value><value>Open-Source</value><value>CPython</value><value>Performance</value></tags></item><item><title><value>Basic Data Science tools</value></title><author><value>Sarah Diot-Girard</value></author><description><value>The aim of this session is to help data scientists with little experience in Python : </value><value>Set their environment development: install Python, Jupyter, set Virtualenv , install scientific libraries: Pandas, Numpy, Scipy.. </value><value>Get started with Jupyter, Pandas  </value><value>Solve practical issues with Pandas and other scientific libraries  </value><value>Get started with Matplotlib and other dataviz tools </value></description><date><value>Thursday 26 July</value></date><tags><value>Visualization</value><value>Data Science</value><value>Jupyter/iPython Notebook</value><value>Scientific Libraries (Numpy/Pandas/SciKit/...)</value></tags></item><item><title><value>Python and Raspberry Pi</value></title><author><value>Ben Nuttall</value></author><description><value>Explore physical computing and more using Python on Raspberry Pi</value><value>The Raspberry Pi is a small affordable computer which runs a Debian-based operating system called Raspbian. It has been designed for the purpose of education, and it is also used by hobbyists and in industry across the globe.</value><value>The Raspberry Pi Foundation is a charity that works to put the power of digital making into the hands of people all over the world by making computing accessible to all. More than 15 million Raspberry Pi computers have been sold since the first product launch in 2012, and all sales profits go towards the Foundation’s educational programmes, courses, and resources.</value><value>The poster covers:</value></description><date><value>Friday 27 July</value></date><tags><value>Internet of Things (IoT)</value><value>Other Hardware</value><value>Raspberry PI</value><value>Education</value><value>Open-Source</value></tags></item><item><title><value>Python Game Console</value></title><author><value>Radomir Dopieralski</value></author><description><value>Learning to make video games is a great way to learn about computers. They are not only very motivating and rewarding, but also teach a very difficult kind of programming — real-time, interactive applications. In the process you also learn about graphics, sound, graphs, artificial intelligence, data structures and algorithms, not because someone tries to teach you, but simply because you actually need all this to make your game. Knowledge gained this way has its own unique quality.
Unfortunately, while Python is an excellent language for learning, it doesn’t make it easy to make games. The number of hoops you have to jump through for installing and configuring everything on every single computer on which you want to run your game makes it really hard to start, and even harder to show your creations to your friends.
To make things easier, I have built a number of devices dedicated to game development, which remove as many of the obstacles as possible to let you simply start on making your game, without distractions.</value></description><date><value>Thursday 26 July</value></date><tags><value>Gadgets</value><value>Teaching</value><value>Education</value><value>Game-Development</value><value>MicroPython</value></tags></item><item><title><value>Plone and modern frontend frameworks</value></title><author><value>Paul Roeland</value></author><description><value>Plone has a long track record in being the premier Python-based web CMS. Yet, recent years have seen an acceleration in frontend technologies. With a stable restful API, it is now possible to use your favorite frameworks like React or Angular to create modern and bespoke frontends and apps, while still using all of  the  stability, security and workflow that a mature product such as Plone provides.  </value><value>Cake. Have. Eat. </value><value>The poster will highlight the state-of-the-art, used-in-production methods, widget libraries and updated UI/UX.</value></description><date><value>Thursday 26 July</value></date><tags><value>CMS</value><value>JavaScript</value><value>RESTful</value><value>Plone</value><value>Open-Source</value></tags></item><item><title><value>What do a model boat and a giant laser have in common?</value></title><author><value>Thomas Kluyver</value></author><description><value>Last year, I was part of a team building a small autonomous sailing boat. This year, I started working at European XFEL, a research facility built around a 3km long X-ray laser. Both projects use Python in their control systems, and despite the vastly different size and budget, they have surprisingly similar architectures, with separate processes sending data to each other using sockets.</value><value>We wrote the code for our sailing boat using an open source framework called the Robot Operating System, or ROS. ROS is built around the concept of communicating nodes: a node can publish messages on a named topic, and any other nodes subscribed to that topic will receive the message. Our boat runs about 15 nodes (the precise number depends on what task it’s doing). This isn’t the only way to drive a robot boat: the team from Aberystwyth, whose code is also open, use a very different architecture.</value><value>European XFEL, in Hamburg, is a new €1.2 billion collaboration between 12 countries. The control and data analysis systems are built on a framework called Karabo, developed in-house. Karabo ‘devices’ - which can encapsulate hardware interfaces or software processing - communicate using both a central message broker and point-to-point channels.</value><value>I’ll describe each of these projects, and the similarities and differences between their software systems. Then I’ll talk about what has been good and bad on each project, and what patterns and ideas we can take from them for designing other systems, including systems that don’t control hardware.</value></description><date><value>Friday 27 July</value></date><tags><value>Science</value><value>Engineering</value><value>Robotics</value><value>Architecture</value><value>Case Study</value></tags></item><item><title><value>introduce Python community/conf, and cross region community in Asia.</value></title><author><value>Noah Chen</value></author><description><value>PSF(Python Software Foundation)  help most people who want to learn Python, and we also join so many meetup/event/conference organized by PSF or related to PSF. in Europe, thanks EuroPython Society doing a lot of jobs in handle or manage so many different region. 
Python is also very popular in Asia. me , Noah who is from Taipei, learn Python and join Taipei.py and PyConTW since 2014. I received the passion from member of Python Community in Taipei, not only in Taiwan, we also have 9 python communities in different cities around Taiwan, included KaoHsung, TaiChung, Tainan, HwaLien, TaoYoung, and we also have 2 female community which called: PyLadies Taipei, and DjangoGirls Taipei. 
in Taipei, we already have so many open source event or conference , what different to Python Community or PyConTW? the reason I want to join and contribute Python is : I can connect the world through Python. PyConTW is found by yyc(Yung-Yu Chen) since 2012. at 2012, PyConTW always invite famous people in Python to present in PyConTW, and also, outstanding speakers always bring their outstanding python friends. PyConTW is the first and most foreigner audiences or speakers international Open Source Conference I joined. this is way I can connect the world, face to face. 
not only PyConTW, I also try to connect PyCon around Asia, like PyCon Japan, and Korea, I am be a volunteer since 2016, and going on. being a volunteer of PyConMY APAC and PyConID in 2017. this is the contact and connect the world we want to share in Python, and want to contribute in Python in Asia.
now , I join FOSSASIA, being a volunteer of FOSSASIA, I can help to promote my experience of Python and FOSS to people in Taipei. and now, I want to share my Asia experience to python friends in Europe.</value></description><date><value>Friday 27 July</value></date><tags><value>Python Software Foundation (PSF)</value><value>Communication</value><value>Conferences and Meet-Ups</value><value>Diversity</value><value>Community</value></tags></item><item><title><value>Making neural networks portable with ONNX</value></title><author><value>Michał Karzyński</value></author><description><value>The world of deep learning frameworks is complex. It seems that every major company has their own product in this space. Some have great features, others have great performance, so its very difficult to choose the right one. But what if you didn’t have to limit your choice to just one? What if you could use the most developer-friendly framework for designing a neural network, the most efficient framework for training and finally the lightest one for inference on edge devices? That’s the idea people at Facebook, Microsoft and Amazon were thinking about when they created the ONNX format.</value><value>Open Neural Network Exchange (ONNX) is a binary file format, based on Protocol Buffers, designed to store representations of neural networks. Networks are stored as computational graphs and the format supports saving both the network architecture and trained weighs.</value><value>ONNX is an open standard and it’s gaining broad community support. Tools already exist for importing and exporting models from frameworks such as Caffe2, PyTorch, Microsoft Cognitive Toolkit (CNTK), Apache MXNet, Chainer, TensorFlow, SciKit-Learn. ONNX support will be built directly into Windows and Apple’s CoreML. Runtimes exist for other environments such as servers and neural network accelerators (Intel’s nGraph), GPUs (NVIDIA’s TensorRT) and more. Other tools such as Netron and Visual DL allow you to visualize and inspect your ONNX models.</value><value>My poster will describe the capabilities of ONNX and demonstrate them through code samples and screenshots. We’ll also cover ONNX’s current limitations and areas of future development.</value></description><date><value>Friday 27 July</value></date><tags><value>Deep Learning</value><value>Data Science</value><value>Open-Source</value><value>Machine-Learning</value></tags></item><item><title><value>Hardening Plaintext Secrets in Configuration Files</value></title><author><value>Moisés Guimarães</value></author><description><value>Many applications and services rely on configuration data in order to behave according to the customer needs. The standard library gives us ConfigParser, and many projects use it to achieve easy configuration with plaintext config files.</value><value>OpenStack Common Libraries (Oslo) has an enhanced alternative called oslo.config with support to command line arguments, option deprecation, and much more. With the addition of the source drivers feature, it is possible to increase the security of config values storing them in a safer place.</value><value>The source drivers feature allows extra sources of configuration data other than plaintext config files adding the possibility to have other layers of security around the configuration values and increasing the fail-safe options.</value></description><date><value>Thursday 26 July</value></date><tags><value>Development</value><value>Best Practice</value><value>Security</value><value>Open-Source</value><value>Command-Line</value></tags></item><item><title><value>Human Mobility Patterns</value></title><author><value>Antonia Tugores</value></author><description><value>Research on mobility has traditionally relied on surveys and datasets generally composed of small samples with a low spatio-temporal resolution. However, the situation is now changing with digital data sources. We use geolocated data from different contexts to analyse human mobility at different levels and predict people fluxes between cities.</value><value>Studies can be done at different levels ranging from intracity to intercontinental trips. Do tourists and residents behave in the same way when in the same city? Are we prone to visit some cities if we have visited another one? What about train and private transport networks, do people from different countries show different patterns? Can we predict people fluxes between cities?</value><value>Being one of the main programming languages when talking about data analysis and data science,  Python and its data analysis ecosystem have been the barebones of this research.</value></description><date><value>Thursday 26 July</value></date><tags><value>Science</value><value>Physics</value><value>Predictions</value><value>Data</value><value>Analytics</value></tags></item><item><title><value>Double the money you give PSF, etc., without paying more!</value></title><author><value>Steve Barnes</value></author><description><value>Many organisations, especially the larger ones, have “matching gifts” programs.</value><value>If you work for such an organisation you can potentially double or more the value of any gifts that you give to the EuroPython Society, (EPS), &amp; Python Software Foundation, (PSF), and others.</value><value>No coding experience is needed just a varying amount of persistence &amp; patience.</value></description><date><value>Wednesday 25 July</value></date><tags><value>Python Software Foundation (PSF)</value><value>EPS</value><value>The Answer to Life the Universe and Everything Else</value></tags></item><item><title><value>Discovering the nuclear reactor’s stability with SLEPc and Python</value></title><author><value>Javier Jorge</value></author><description><value>Different engineering models such as physic simulations or information retrieval rely on modelling and solving large-scale sparse eigenvalue problems, such as fluid simulation or document retrieval. SLEPc (http://slepc.upv.es) is a software library for the solution of this kind of algebraic problems on distributed computers. We can use this with Python through slepc4py to provide solutions to these computationally expensive problems, using parallelization with different schemes.</value><value>In this poster we introduce slepc4py (https://bitbucket.org/slepc/slepc4py), a python wrapper for SLEPc, along with the problem of determining the nuclear reactor’s stability, a problem that is modeled as obtaining the eigenvalues of certain matrices that are large and sparse. We introduce the techniques that are implemented in SLEPc for solving a problem with these characteristics.</value></description><date><value>Friday 27 July</value></date><tags><value>Scientific Libraries (Numpy/Pandas/SciKit/...)</value><value>Case Study</value><value>Data</value><value>C-Languages</value><value>Science</value></tags></item><item><title><value>Create your own Artificial Intelligence to monitor your Linux System!</value></title><author><value>Maha Mdini</value></author><description><value>The object of this session is to explain how one may apply simple statistical calculations and Machine Learning techniques to monitor one’s Linux system. Systems, running services and installed applications generate a large amount of logs. One may create also customized logs for a particular purpose. </value><value>These logs may be processed in real time or in demand by the means of smart Python scripts for varied purposes:</value><value>1- Optimizing the performance of the system by monitoring the systems logs  (e.g. boot logs) and modeling metrics such as CPU/memory usage, monitoring the performance of services such as HTTP,  MySQL…</value><value>2- Securing the system from external threats by monitoring browsing, ports, login logs …, as well as from internal crashes by monitoring kernel logs</value><value>3- Modeling one’s daily behavior by measuring the frequency/correlations of the usage of applications/services …  </value></description><date><value>Thursday 26 July</value></date><tags><value>Scientific Libraries (Numpy/Pandas/SciKit/...)</value><value>Data Science</value><value>Open-Source</value><value>Linux</value></tags></item><item><title><value>Fifty Shades of Disaster - Human Error and Complex Systems</value></title><author><value>Paul Ross</value></author><description><value>This four part tutorial describes the failures that can happen when humans interact with complex systems. Complete with case studies, this gives valuable lessons for developers, project managers and DevOps engineers. Learn from the mistakes of others as you won’t live long enough to make them all yourself and this talk gives you that opportunity!</value><value>Part 1: Complexity, Coupling and Systems Failures
A gentle introduction to “modern accident theory”. We examine the essential characteristics of complex systems and the operators who control them. A life hack is here along with two case studies of software disasters.</value><value>Part 2/4: A Concise History of Civil Aviation
Civil aviation has moved from very risky to extraordinarily safe. How this was done has valuable lessons for other industries. We looks at the challenges faced by civil aviation, how they were overcome and what we could learn from this.</value><value>Part 3/4: Blame and the Fallacy of Root Cause Analysis
So disaster has finally happened, now, how do you go about preventing futures disasters? The obvious ways are wrong. So how do you investigate failure and how do you apply those lessons?</value><value>Part 4/4: Skill, Luck and Sheer Professionalism
Sometimes the human element, the operators, are not hapless perpetrators complicit in the disaster but actively prevent catastrophe. This talk is full of case studies where this happened. An analysis of these case studies will help you to improve your own resilience.</value></description><date><value>Wednesday 25 July</value></date><tags><value>Operations</value><value>Case Study</value><value>Best Practice</value><value>DevOps general</value><value>failures/mistakes</value></tags></item><item><title><value>Cloud-Agnostic Deployment of Distributed TensorFlow</value></title><author><value>Javier Jorge</value></author><description><value>In this poster we will see how to deploy the required infrastructure to create a TensorFlow cluster, and then provision the software to train the model. For doing this, we will use the Infrastructure Manager (http://www.grycap.upv.es/im/index.php) that supports API’s from different virtual platforms, making user applications Cloud-agnostic. In addition, it integrates a contextualization system, based on Ansible, to enable the installation and configuration of all the required applications providing a fully functional Deep Learning infrastructure on the Cloud provider that we need.</value><value>Outline: </value></description><date><value>Thursday 26 July</value></date><tags><value>Science</value><value>Deep Learning</value><value>Scientific Libraries (Numpy/Pandas/SciKit/...)</value><value>Machine-Learning</value><value>Case Study</value></tags></item><item><title><value>Best practices for elegant experimentation in data science projects (case study)</value></title><author><value>K K</value></author><description><value>In the course of the project, data scientists face multiple issues. Difficulties with reproducibility, lack of the ability to prepare experiments quickly and dirty data are just three examples. Data science projects involve a lot of experimentation and quick adoption of new ideas and technologies. Such environment makes it difficult to keep the code clean as well as keep track of small changes that makes new experiment successful.</value><value>Here, we use an instance segmentation challenge - called Mapping Challenge - hosted on the crowdAI platform to show: 1) our best practices when working in data science projects, 2) competition results. Our best practices involve usage of the steppy library, which provides minimal interface for building machine learning pipelines. Besides this, we organized our work in a transparent and open way, publishing code, tasks and experiments results.</value><value>On the poster, we share our results regarding pre- and post-processing routines, network architectures and training scheme. We also present technology stack that we use. It is a blend of well established Python packages (like numpy and sklearn) and our own open source initiatives, that is steppy and steppy-toolkit.</value><value>Poster is for Pythonists looking for: 1) example solution to the instance segmentation task, 2) ideas how to organize data science project.</value></description><date><value>Friday 27 July</value></date><tags><value>Best Practice</value><value>Data Science</value><value>Clean Code</value><value>Open-Source</value><value>Machine-Learning</value></tags></item><item><title><value>Asynchronous operations at scale</value></title><author><value>Hubert Bryłkowski</value></author><description><value>This talk will cover main purpose, differences, advantages and shortcoming of synchronous and asynchronous operations. Using real life  infrastructure as an example, Hubert will demonstrate how actions like adding a new content can be processed in a non-blocking way. Such approach is more taxing on development time but can be highly beneficial if your application has to sustain high access rate and respond swiftly.
By attending the talk you will learn how we build application that support &gt;100M monthly users, what problem arise in such high pace environments and what trade off application developer has to make.</value></description><date><value>Wednesday 25 July</value></date><tags><value>Performance</value><value>Architecture</value><value>ASYNC / Concurreny</value><value>Multi-Processing</value><value>Microservices</value></tags></item><item><title><value>Writing and Running Tests in Docker</value></title><author><value>Alexandre Figura</value></author><description><value>“How to Launch your Tests with Docker Compose: Best Practices and Traps to Avoid": that would make a great title for a book! Unfortunately, there is no such book available currently. In the meantime, we are all struggling on how to make our applications running in Docker… Because there is so many ways to do it, it’s often hard to find out what is the best way to do it.</value><value>But with some practice, and many trials/errors, some patterns take shape. That’s what we will see during this training session. We will start by writing some tests for a web application with Pytest. Then, we will automate them with Tox. And to finish, we will run them in Docker Compose. Our development workflow will be managed with Invoke, and our goal will be to have tests independent of the running environment, so they can be run both locally or on a continuous integration server (e.g., GitLab with Kubernetes runners).</value><value>Requirements:</value></description><date><value>Tuesday 24 July</value><value>Tuesday 24 July</value></date><tags><value>Best Practice</value><value>Tooling</value><value>Docker</value><value>Deployment/Continuous Integration and Delivery</value><value>Test Libraries (pyTest/node/...)</value></tags></item><item><title><value>Technologies to master Parallelism in Python</value></title><author><value>Shailen Sobhee</value></author><description><value>Parallelism in Python has been a mysterious pinnacle of language mastery for many years, and with a few advancements in the community it has become even easier to do so.  From easy imported threadpool controls, to advanced multiprocessing affinity and pinning techniques, the options to achieve parallelism are now quite within reach.  In this workshop, learn how to use these frameworks to augment one’s Python code to enable granular control over parallelism workloads and combat oversubscription.</value><value>Slides: https://www.scribd.com/document/384826546/EuroPython-2018-Shailen-Sobhee</value></description><date><value>Tuesday 24 July</value><value>Tuesday 24 July</value></date><tags><value>Performance</value><value>Multi-Threading</value><value>Scientific Libraries (Numpy/Pandas/SciKit/...)</value><value>Multi-Processing</value></tags></item><item><title><value>Ridiculously Advanced Python</value></title><author><value>Francesco Pierfederici</value></author><description><value>If you have been using Python for some time already and want to reach new heights in your language mastery, this training session is for you!</value><value>Python has a number of features which are extremely powerful but, for some reason are not particularly well known in the community. This makes progressing in our Python knowledge quite hard after we reach an intermediate level. Fear not: this session has you covered!</value><value>We will look at some advanced features of the Python language including properties,  class decorators, the descriptor protocol, annotations, data classes and meta-classes. If time allows we will even delve into the abstract syntax tree (AST) itself. </value><value>We will use Python 3.7 and strongly recommend that attendees install a reasonably recent version of Python 3 to make the most out of the training.</value><value>Warning: some of the topics presented will almost certainly assure an early end to an otherwise successful career in software engineering :-)</value><value>Source code available on GitHub: https://github.com/pythoninside/europython2018</value></description><date><value>Monday 23 July</value><value>Monday 23 July</value><value>Monday 23 July</value></date><tags><value>Software Design</value><value>Type-Hinting</value><value>Meta Classes</value><value>Programming</value><value>Python 3</value></tags></item><item><title><value>Privacy for Data Scientists</value></title><author><value>Andreas Dewes</value></author><description><value>As data and information security become core components of managing user data, data scientists are keen to expand their knowledge and skills relating to data privacy and security basics. As of May 2018, the European General Data Protection Regulation affects how European residents can access and grant consent to use their data. As European data scientists, we now have an obligation as well as distinct motivation, to practice data science with attention to data privacy.</value><value>In this workshop, we will introduce some of the basics in terms of defining privacy within the realm of data collection, modeling and machine learning. A focus on practical knowledge and code, we will cover how one can implement some of these algorithms with Python. Students will be presented with these theories along with recent research on privacy-preserving models, so they can leave with a better understanding of how to apply privacy principles to data science in their work and study.</value></description><date><value>Tuesday 24 July</value><value>Tuesday 24 July</value></date><tags><value>Security</value><value>Data</value><value>Jupyter/iPython</value><value>Machine-Learning</value><value>Privacy</value></tags></item><item><title><value>Introduction to Pandas, Testing and Test-Driven Data Analysis</value></title><author><value>Nick Radcliffe</value></author><description><value>This is replacement training for Sandrine’s Intro to Pandas and SciKit Learn, which she sadly won’t be able to present because she has recently broken her nose!</value><value>This training will introduce Pandas (and Numpy) for people who either haven’t used them at all or are just getting started with them. It will then go on to introduce testing in Python (in general), with unittest, and testing analytical code, in particular, with the test-driven data analysis package.</value><value>Planned content:</value><value>Pandas and Numpy:
  - what they are
  - why you should used them
  - how to use them
  - basic numpy concepts (arrays, dtypes, array operations, zeros, ones, creating new arrays, where, choose, sum, size etc.)
  - pandas basics: DataFrames, creating DataFrames, loading data, indices
  - creating new columns, appending DataFrames, merging DataFrames, groupby operations, plotting.
  - loading and saving data (pickle; csv, feather)</value><value>Testing
  - the idea of testing and test-driven development
  - the unittest library basics: unittest.TestCase, unittest.main, verbosity, self.assertEqual and friends.
  - slightly advanced unittest basics: setUp, tearDown, setUpClass, tearDownClass
  - unit tests vs. integration and system tests
  - pytest as an alternative to unittest</value><value>Test-Driven Data Analysis:
  - making the case for testing analytical processes
  - extending the ideas from software testing to reference tests for analytical processes with ReferenceTestCase from the TDDA library
  - Using constraints to test data (input; output; intermediates).
       - Automatically generating constraints from known good data
       - Verifying data against generated (or hand-created) constraints 
       - Detecting bad data (and anomalies)</value><value>PREPARATION:</value><value>As a bare minumum, you will need a working Python (preferably 3.6 or 3.7, though 2.7 could be used) with numpy and pandas installed, preferably with JupyterLab. Anaconda is a great way to get all this.</value><value>Ideally, you should also have installed the TDDA library, the feather library and the pmmif library in case the network is overloaded during training.</value><value>All dependencies are available from Github:</value></description><date><value>Monday 23 July</value><value>Monday 23 July</value></date><tags><value>Test Driven Development (TDD)</value><value>Data Science</value><value>Testing</value><value>Scientific Libraries (Numpy/Pandas/SciKit/...)</value></tags></item><item><title><value>Real-time transcription and sentiment analysis of audio streams</value></title><author><value>Aaron Bassett</value></author><description><value>In this training session, we’re going to learn how to create a virtual rapporteur. A digital assistant who can join any conference call; record it and provide participants with real-time insights into the overall tone of the call. Once the call is complete, we’ll look at how we can use the call recording to provide participants with a text transcript as well as meta information about the call such as the most talked about concepts, keywords and entities.</value><value>Attendees</value><value>: you should be familiar with Python and the command line.  You will also need to sign-up for a free </value><value>Nexmo</value><value> and </value><value>IBM Watson</value><value> account to access their APIs.</value><value>We’ll be coding the application in Python and JavaScript, with the </value><value>Hug</value><value>, </value><value>Tornado</value><value> frameworks; so a knowledge of both languages would be beneficial but is not required. We will be making heavy use of several APIs, so experience with REST and WebSockets will help.</value></description><date><value>Monday 23 July</value><value>Monday 23 July</value></date><tags><value>Visualization</value><value>JavaScript</value><value>Natural Language Processing</value><value>Machine-Learning</value><value>Web Servers and MicroFWs (Flask/Tornado/Nginx/...)</value></tags></item><item><title><value>Get your documentation right</value></title><author><value>Daniele Procida</value></author><description><value>This workshop introduces, and helps you implement, an approach that </value><value>will</value><value> improve your project’s documentation.</value><value>This approach focuses on documentation </value><value>structure</value><value>. Based on sound, well-established principles concerning a number of topics (including learning, understanding, praxis, pedagogy amongst others) it provides a documentation framework that is easy to understand, straightforward to put into practice, and above all, immediately and enduringly effective.</value><value>In the workshop we’ll use a hypothetical documentation set for examples, but attendees should have some actual documentation - whether already extant or in the planning phase - to apply it.</value><value>The principles used in this workshop are outlined at https://www.divio.com/en/blog/documentation/. This work has been presented in numerous conference talks, such as https://www.youtube.com/watch?v=t4vKPhjcMZg.</value><value>It has been applied successfully to many software projects, making the documentation better for users, and more manageable for its maintainers.</value></description><date><value>Tuesday 24 July</value><value>Tuesday 24 July</value></date><tags><value>Documentation</value></tags></item><item><title><value>Fast native code with Cython</value></title><author><value>Stefan Behnel</value></author><description><value>Cython</value><value> is not only an excellent and widely used tool to speed up computational Python code, it’s also a very comfortable way to talk to native code and libraries. The Cython compiler translates Python code to C or C++ code, and supports static type annotations to allow direct use of C/C++ data types and functions. The tight integration of all three languages makes it possible to freely mix Python features like generators and comprehensions with C/C++ features like native data types, pointer arithmetic or manually tuned memory management in the same code.</value><value>This tutorial by a core developer introduces the Cython compiler by interactive code examples, and shows how you can make it generate fast binary modules that talk to native libraries, as easily as Python itself.</value><value>Attendees should make sure they have a C compiler installed and configured to build binary Packages for CPython. We will use CPython 3.6/7, although all examples can also be followed with CPython 2.7. If you’re not on Linux, I recomment using a </value><value>conda</value><value> environment with gcc.</value><value>Update 2018-07-23</value><value>: Please set up a virtualenv or conda-env with the following tools:</value></description><date><value>Tuesday 24 July</value><value>Tuesday 24 July</value></date><tags><value>Cython</value><value>Compiler and Interpreters</value><value>C-Languages</value><value>Performance</value></tags></item><item><title><value>Introduction to Big Data Processing using Spark and Python</value></title><author><value>Raoul-Gabriel Urma</value></author><description><value>This workshop will provide a hands-on introduction to the Big Data ecosystem, Hadoop and Apache Spark in practice. Through practical activities in Python, you will learn how to apply Apache Spark on a range of datasets to process and analyse data at scale. </value><value>After taking this workshop you will be able to: </value><value>SETUP
Download / Clone the repository: http://gitlab.cambridgespark.com/pub/bigdata-spark</value><value>Follow the instructions in the SETUP.md file: http://gitlab.cambridgespark.com/pub/bigdata-spark/blob/master/SETUP.md</value></description><date><value>Monday 23 July</value><value>Monday 23 July</value></date><tags><value>Data Science</value><value>Big Data</value><value>Jupyter/iPython</value></tags></item><item><title><value>Data Wrangling &amp; Visualisation with Pandas &amp; Jupyter</value></title><author><value>Alexander Hendorf</value></author><description><value>One of the best tools around for data wrangling and analysis in Python is is Pandas. With Pandas dealing with data-analysis is easy and simple but there are some things you need to get your head around first as Data-Frames and Data-Series. </value><value>After this tutorial you will be able to work with Pandas and make simple data analytics incl. visualisations. 
Pandas is not only useful in data science it’s also a great tool for creating e.g. sales reports or any other data-driven report required in business.
 It’s easy to make fancy analytics while integrating with fellow co-workers used working with Excel.</value><value>Please do come prepared and follow these simple </value><value> installation instructions</value><value>Pandas features directly accessible, powerful visualisations.</value><value>The workshop will be provided as Jupyter notebook for the attendees to follow along.</value></description><date><value>Tuesday 24 July</value><value>Tuesday 24 July</value></date><tags><value>Scientific Libraries (Numpy/Pandas/SciKit/...)</value><value>Data Science</value><value>Business</value><value>Analytics</value></tags></item><item><title><value>Build, Deploy, Win with Serverless Applications</value></title><author><value>Kyle Knapp</value></author><description><value>Serverless applications make it easy for you to run your code without managing servers. Serverless applications can range anywhere from running web applications, to real-time data processing systems, to cron jobs in the cloud. Taking a hands-on approach, this tutorial will teach you how to write and deploy serverless applications in a matter of minutes! No prior experience with serverless applications needed.</value><value>Using AWS Chalice, a Python serverless microframework for AWS, you will be developing serverless applications that run on AWS Lambda, a service run that lets you run code without provisioning or managing servers. In the process of developing these serverless applications, you will learn how to:</value><value>•   Quickly and effectively develop and deploy serverless applications</value><value>•   Utilize best practices in developing serverless applications</value><value>•   Build a wide range of serverless applications by leveraging other AWS services alongside AWS Lambda</value><value>No prior experience in serverless application or AWS is needed, but please make sure your development environment meets the prerequisites of the workshop:</value><value>•   Python 2.7 or 3.6</value><value>•   Virtualenv</value><value>•   AWS credentials</value><value>•   git</value><value>More information on environment setup can be found in the following link: 
https://chalice-workshop.readthedocs.io/en/latest/env-setup.html</value></description><date><value>Monday 23 July</value><value>Monday 23 July</value></date><tags><value>Public Cloud (AWS/Google/...)</value><value>Web</value><value>Best Practice</value><value>APIs</value><value>Microservices</value></tags></item><item><title><value>Building your own sports trading bot</value></title><author><value>Tomasz Dziopa</value></author><description><value>In this workshop participants will get a glimpse into the world of automated sports trading. The workshop will start out with an introduction into the realm of betting exchanges, trading bots and the strategies that can be employed to successfully trade sports. After the introduction we will move into a more “hands on” session where everyone will get API access to the Smarkets betting exchange and a skeleton of a trading bot which can then be extended with the ultimate goal of creating your first sports trading bot!</value><value>The requirements for this workshop will be basic knowledge of Python and RESTful APIs. Basic knowledge of statistics would also be helpful but is not essential. Last but not least: Do bring your laptops!</value></description><date><value>Tuesday 24 July</value><value>Tuesday 24 July</value></date><tags><value>python</value><value>Trading</value><value>APIs</value></tags></item><item><title><value>Writing good error messages</value></title><author><value>Paul Keating</value></author><description><value>Anyone who has ever conducted an elementary programming course, or even answered a question on StackOverflow, will know that reading error messages is a skill that beginners have to learn. 
It is less widely appreciated that writing good error messages is also a skill that must be learnt.
This talk is in two parts. 
The first covers the commonest error message gaffes:
•   Insufficiently explicit messages.
•   Issuing the same message for two different conditions.
•   Suppressing the stack trace.
•   Polluting the stack trace.
The second part describes a way to ensure usable, actionable error messages, even when the writer of the message is not a professional coder. This was developed for an environment where superusers code up most of the dozens of data validation rules and the accompanying messages, and the application in which Python is embedded suppresses the stack trace.</value></description><date><value>Friday 27 July</value></date><tags><value>Python general</value><value>Programming</value></tags></item><item><title><value>Best Practices for a Blazing Fast Machine Learning Pipeline</value></title><author><value>David Liu</value></author><description><value>Getting through preprocessing to analytics to insights quickly has been the crux of many a Data Scientist.  In this workshop, learn about the best tools, techniques, and frameworks that allow for a fast machine learning pipeline—topics covered will include preprocessing vectorization tricks, distributed dataframe handling, along with tips and tricks to help scale out one’s machine learning code out to cloud or clusters. </value><value>Workshop Overview:</value><value>-Introduction</value><value>-Tools and Techniques</value><value>-Data preprocessing</value><value>-Break (15 min)</value><value>-Data Visualization</value><value>-Machine Learning</value><value>-Options for scaling and pipelining</value><value>-Break (15 min)</value><value>-Hands-on: Advanced tools</value><value>-Hands-on: Chaining it together</value><value>-Prerequisites are from the two Github repos utilized for the training: (slides in repo)https://github.com/triskadecaepyon/ep_2018_workshop  and https://github.com/IntelPython/workshop</value></description><date><value>Monday 23 July</value><value>Monday 23 July</value></date><tags><value>Scientific Libraries (Numpy/Pandas/SciKit/...)</value><value>Data Science</value><value>Machine-Learning</value></tags></item><item><title><value>Winning card games with 1000+ CPUs</value></title><author><value>vincent warmerdam</value></author><description><value>Vincent was playing a card game against his girlfriend and he kept loosing. So he wanted to train a bot to play on his behalf. This is our story.</value><value>We’re using AWS Lambda to get better at a card game named SushiGO. We make a small genetic algorithm in Python that uses AWS Lambda as a backend. The talk consists of these parts:</value><value>This talk will discuss an algorithm that we’ve tried to improve in three ways:</value><value>We will conclude by discussing whether or not AWS Lambda is suitable for a gridsearch/grid simulation (hint, it’s not meant for this task, but it actually kind of works very well).</value></description><date><value>Friday 27 July</value></date><tags><value>Predictions</value><value>Algorithms</value><value>ASYNC / Concurreny</value><value>Command-Line</value><value>Python 3</value></tags></item><item><title><value>A Python-flavored Introduction to Containers And Kubernetes</value></title><author><value>Ruben Orduz</value></author><description><value>NOTE: This workshop is for very beginners. If you already have had experience with containers and/or Kubernetes, you likely not get much out of this training.</value><value>While not required, if you want to follow along, please follow this link below to set your environment up:
</value><value>requirements</value><value>Containers have more or less taken over the world of application, web APIs, mobile endpoints and other forms of deployment. They have become the currency, the “table stakes” and de-facto application deployment unit. Their raise to the fore has brought about a whole host of use cases which weren’t practical or accessible in the world of “classic” paradigms of infrastructure and virtualization. Containers have also brought application deployment closer and more accessible to developers. </value><value>But as more use cases, deployment styles and exponential adoption of containers was ongoing, a new set of problems began to surface: how do you manage the ever growing number of containers in a deployment? How do you make sure containers have the right resources, deployed to the right machine, running with the correct parameters, how do you scale in and out without disruption? How do you make sure in a fleet of X containers that they’re all running and in healthy state? Enter Kubernetes.</value><value>The goal of this course is to give the attendees a solid foundation of the core concepts mentioned above both in theory and hand-on practice using docker, kubernetes and friends to deploy a sample multi-tier python web application. All attendees need is a laptop with minikube installed and an Internet connection.</value><value>Full course outline and abstract (because character limit) </value><value>here</value><value>. </value><value>This training is an up-to-date version of the training delivered at last year’s EuroPython</value></description><date><value>Monday 23 July</value><value>Monday 23 July</value></date><tags><value>Docker</value><value>DevOps general</value></tags></item><item><title><value>Why develop a CLI (Command Line Interface) first?</value></title><author><value>Steve Barnes</value></author><description><value>One of the core concepts of Application Development, (not just in python), is the separation between the Business Logic and the User Interface. However there is a strong temptation to start with the user interface and add the business logic to it some methodologies emphasize this with the process of prototyping the (G)UI first. The danger is that your business logic code can get too entangled with the UI and a change of platform or framework becomes almost impossible.</value><value>This presentation will show how to maintain a clear separation between the Business Logic and the User Interface by starting with a command line interface using argparse and growing a GUI on top.</value><value>We will cover:
 - Why maintain the seperation
 - Using argparse
 - Adding a GUI layer with wxPython
 - Automating the GUI generation
 - Adding a web interface
 - Testing advantages of this approach
 - Scripting advantages
 - Some packaging models.</value><value>Slides and Samples all uploaded to https://github.com/GadgetSteve/EP2018_Talk</value></description><date><value>Thursday 26 July</value></date><tags><value>Software Design</value><value>Development</value><value>Packaging</value><value>Command-Line</value><value>Cross-Platform-Development</value></tags></item><item><title><value>When to use Machine Learning: Tips, Tricks and Warnings</value></title><author><value>Pascal van Kooten</value></author><description><value>Artificial Intelligence, and machine learning in particular, is one of the hottest topics in tech/business.
I will explain the core of machine learning, and the main goal of this talk will be to help you judge the success whenever someone yells “I know! let’s solve this using machine learning!”. I will also provide tips and tricks on how to increase the success of such projects. The second part of the talk will be about 2 open-source python projects I’ve created, as well as a project I’m working on regarding the trading of cryptocurrency… and their relation to machine learning. Specifically, the challenges and findings in making these cases work will be explored.</value></description><date><value>Thursday 26 July</value></date><tags><value>Best Practice</value><value>failures/mistakes</value><value>Open-Source</value><value>Machine-Learning</value><value>Use Case</value></tags></item><item><title><value>What's new in Python 3.7</value></title><author><value>Stephane Wirtel</value></author><description><value>Scheduled for release in mid-June before the conference, Python 3.7 is shaping up to be a feature-packed release! 
This talk will cover all the new features of Python 3.7, including the Data Classes and the Context Variables for the asynchronous programming with asyncio.</value></description><date><value>Wednesday 25 July</value></date><tags><value>Python general</value><value>ASYNC / Concurreny</value><value>CPython</value></tags></item><item><title><value>Descriptors and Metaclasses - Understanding Python's More Advanced Features</value></title><author><value>Mike Müller</value></author><description><value>Descriptors and metaclasses are advanced Python features. While it is possible to write Python programs without active knowledge of them, knowing more about them facilitates a deeper understanding of the language. With examples, you will learn how they work and how to write your own descriptors and metaclasses. Furthermore, you will understand when to use and when better not to use them.</value><value>This tutorial is a systematic introduction to descriptors and metaclasses. It covers all relevant information with a focus on practical applications for common tasks.</value><value>In hand-on sessions you will learn how to write your own descriptors that adapt attribute access to your needs. You will experience how metaclasses can help you to get more insight into a code base.</value><value>Use cases provide working code that can serve as a basis for your own solutions. You will gain a deeper understanding of more advanced concepts that can help to write better programs.</value><value>The training is designed for Python 3 (3.6+ preferred). Most of the material will work with Python 2.7, sometimes requiring minor modifications. I will use  a </value><value>Jupyter Notebook</value><value> in [Jupyterlab] ( https://github.com/jupyterlab/jupyterlab) in the training. I encourage you to try them out. Otherwise any editor, IDE, or interactive Python environment your prefer works. No other 3rd party libraries to install. Please download the </value><value>course material</value><value>.
I’ve been delivering these topics over the last years as a part of numerous advanced corporate trainings and open courses as well as trainings at PyCon US, EuroPython, PyCon PL, PyCon DE, and PyCon IE. The material has been continuously refined based on the  feedback from participants.</value></description><date><value>Tuesday 24 July</value><value>Tuesday 24 July</value></date><tags><value>Best Practice</value><value>Python general</value><value>Programming</value><value>Use Case</value></tags></item><item><title><value>Washing away code smells</value></title><author><value>Yenny Cheung</value></author><description><value>Does your code smell? Have a weird fragrance? It turns out code smells are a real thing and an amazing conceptualization of suboptimal design. This talk helps you identify code smells in Python. It also shows you how to wash them away by the technique of refactoring. You will learn the art of writing Pythonic, clean and maintainable code.</value><value>Code smells refer to the symptoms of problematic code design. Identifying different types of code smells is the first step to successful refactoring. I will talk through some classic examples:</value><value>Knowing what to refactor, I will share a few learnings that lead to good quality code:</value><value>I will also share tips on using refactoring at your company, which includes convincing your product manager, looking out for code smells during code reviews, and employing automatic tools.</value><value>The speaker has previously presented on Talk Python To Me Podcast: https://talkpython.fm/episodes/show/150/technical-lessons-learned-from-pythonic-refactoring, and at PyCon.DE: https://www.youtube.com/watch?v=Yq9-b2JKUyU.</value></description><date><value>Wednesday 25 July</value></date><tags><value>Beginners</value><value>Best Practice</value><value>legacy-code</value><value>Clean Code</value><value>Software Design</value></tags></item><item><title><value>What makes coding for MicroPython different?</value></title><author><value>Andrey Vlasovskikh</value></author><description><value>A microcontroller unit is a CPU, memory modules, and I/O devices on a single chip. There are tens of billions of microcontrollers in world: they are everywhere from watches to cars. Developers program them mostly in C, since their tiny hardware resources make it very hard to use higher-level langauges such as Python. In 2014 the MicroPython project was started with the goal of making it possible to program microcontrollers in Python.</value><value>Was it hard to make Python work on a device with only 16 KB of RAM? Is MicroPython a Python dialect or is it a different language? I’ll start with the most important optimizations and the key differences in the language implementation to give you an idea of what MicroPython really is.</value><value>Most of us are not contributors to MicroPython though and it’s much more important for us to understand how Python coding for microcontrollers is different from, say, web development or scripting. What do you need to learn in order to program your mictrocontroller-based IoT devices? I’ll talk about several traits of Python coding for microcontrollers that make it different, but exciting to learn.</value></description><date><value>Wednesday 25 July</value></date><tags><value>Internet of Things (IoT)</value><value>MicroPython</value><value>Programming</value><value>Tooling</value><value>Architecture</value></tags></item><item><title><value>Walking the Random Forest and boosting the trees</value></title><author><value>Kevin Lemagnen</value></author><description><value>Deep Learning is all the rage, but ensemble models are still in the game. With libraries such as the recent and performant LightGBM, the Kaggle superstar XGboost or the classic Random Forest from scikit-learn, ensembles models are a must-have in a data scientist’s toolbox. They’ve been proven to provide good performance on a wide range of problems, and are usually simpler to tune and interpret. This talk focuses on two of the most popular tree-based ensemble models. You will learn about Random Forest and Gradient Boosting, relying respectively on bagging and boosting. This talk will attempt to  build a bridge between the theory of ensemble models and their implementation in Python.</value><value>Notebook: </value><value>https://github.com/klemag/europython2018_walking_the_random_forest</value></description><date><value>Friday 27 July</value></date><tags><value>Case Study</value><value>Data Science</value><value>Machine-Learning</value><value>Scientific Libraries (Numpy/Pandas/SciKit/...)</value></tags></item><item><title><value>Using Pandas and Dask to work with large columnar datasets  in Apache Parquet</value></title><author><value>Peter Hoffmann</value></author><description><value>Apache Parquet is a binary, efficient columnar data format. It uses various
techniques to store data in a CPU and I/O efficient way like row groups,
compression for pages in column chunks or dictionary encoding for columns.
Index hints and statistics to quickly skip over chunks of irrelevant data
enable efficient queries on large amount of data.</value><value>Apache Parquet files can be read into Pandas DataFrames with the two libraries
fastparquet and Apache Arrow. While Pandas is mostly used to work with data
that fits into memory, Apache Dask allows us to work with data larger then memory
and even larger than local disk space. Data can be split up into partitions
and stored in cloud object storage systems like Amazon S3 or Azure Storage.</value><value>Using Metadata from the partiton filenames, parquet column statistics and
dictonary filtering allows faster performance for selective queries without
reading all data. This talk will show how use partitioning, row group skipping 
and general data layout to speed up queries on large amount of data.</value></description><date><value>Wednesday 25 July</value></date><tags><value>python</value><value>Scientific Libraries (Numpy/Pandas/SciKit/...)</value><value>Big Data</value><value>NoSQL</value><value>Databases</value></tags></item><item><title><value>Using Bonobo, Airflow and Grafana to visualize your business</value></title><author><value>Romain Dorgueil</value></author><description><value>Zero-to-one hands-on introduction to building a business dashboard using Bonobo ETL, Airflow, and a bit of Grafana (because graphs are cool).</value><value>There is no need of prior knowledge about any of those tools.</value><value>After a short introduction about the tools, we’ll go through the following topics, using the real data of a small SaaS software:</value><value>One can expect to be able to build a similar system at the end of the talk in a few days (of course, the implementation is only a small part of this process, data is what really matters).</value><value>«Metrics you watch tend to improve over time»</value></description><date><value>Wednesday 25 July</value></date><tags><value>Case Study</value><value>Analytics</value><value>Open-Source</value><value>Business</value><value>Databases</value></tags></item><item><title><value>Understanding and Applying CQRS</value></title><author><value>Vinicius Pacheco</value></author><description><value>Creating scalable applications has a number of complex variables and one of them is to work with scalability and performance in the database layer. Command Query Responsibility Segregation (CQRS) is a design pattern that helps produce more performance and resilience in applications where data access is intense.
In this talk, we will understand when to use and the problems that CQRS solves. We will also apply CQRS in a Python application using the Nameko framework. </value><value>The outline talk is:
        ○ (4 minutes) - Present a real problem of a web application, when creating new instances is not a solution, because the database receives an overwrite of writing and unfeasible to read the data, collapsing the application. 
        ○ (6 minutes) - Present the CQRS pattern conceptually and how this design pattern solves this type of problem using the structure of Command Stack and Query Stack 
        ○ (3 minutes) - Show Nameko as an interesting tool to apply the CQRS. It will demonstrate the use of HTTP, RPC and the possibility of applying pub/sub. 
        ○ (6 minutes) - Create (live code) the Command Stack layer using Nameko on a Postgresql database.
        ○ (5 minutes) - Create (live code) the Query Stack layer using Nameko over a MongoDB database. 
        ○ (3 minutes) - Explain common myths and mistakes about CQRS 
(3 minutes) - Q &amp; A Session </value></description><date><value>Thursday 26 July</value></date><tags><value>Software Design</value><value>python</value><value>Architecture</value></tags></item><item><title><value>Trust me, I'm a Data Scientist - ethics for builders of data-based applications</value></title><author><value>Sarah Diot-Girard</value></author><description><value>Data Science is gonna save the world, right? Or is it? 
Machine Learning epic fails are being largely commented. It’s easy to convince ourselves that they are due to the inconsiderate misuse of Data Science. But is it really so? Is it possible that innocuous choices lead an honnest team to a disaster?</value><value>During the course of this talk, we will build together an (imaginary) application: a disruptive AI-based smart virtual assistant, pledging to help high-schoolers with their university choice. We will see how unintended biaises may creep in at every step, even with the best of intentions. We will explore different topics, such as algorithmic fairness, model interpretability and the handling of minority classes.</value><value>Through this practical example, this talk will present a review of major ethical pitfalls identified in the Machine Learning community along with suggestions on how to avoid them.</value><value>This talk is intended for beginner to intermediate Data Scientists, and people working with Data Scientists, even without specific technical knowledge.</value><value>Slides : https://sdgjlbl.github.io/Presentations/Data%20Science%20and%20Ethics/presentation.html#/</value></description><date><value>Friday 27 July</value></date><tags><value>Data Science</value><value>Machine-Learning</value></tags></item><item><title><value>Understanding and Implementing Recurrent Neural Networks using Python</value></title><author><value>Anmol Krishan Sachdeva</value></author><description><value>Recurrent Neural Networks (RNNs) have become famous over time due to their property of retaining internal memory. These neural nets are widely used in recognizing patterns in sequences of data, like numerical timer series data, images, handwritten text, spoken words, genome sequences, and much more. Since these nets possess memory, there is a certain analogy that we can make to the human brain in order to learn how RNNs work. RNNs can be thought of as a network of neurons with feedback connections, unlike feedforward connections which exist in other types of Artificial Neural Networks.</value><value>The flow of talk will be as follows: - Self Introduction - Introduction to Deep Learning - Artificial Neural Networks (ANNs) - Diving DEEP into Recurrent Neural Networks (RNNs) - Comparing Feedforward Networks with Feedback Networks - Quick walkthrough: Implementing RNNs using Python (Keras) - Understanding Backpropagation Through Time (BPTT) and Vanishing Gradient Problem - Towards more sophisticated RNNs: Gated Recurrent Units (GRUs)/Long Short-Term Memory (LSTMs) - End of talk - Questions and Answers Session</value></description><date><value>Wednesday 25 July</value></date><tags><value>Visualization</value><value>Data Science</value><value>Developing with Python Track</value><value>Machine-Learning</value><value>Deep Learning</value></tags></item><item><title><value>The Web is Terrifying! Using the PyData stack to spy on the spies.</value></title><author><value>Sarah Bird</value></author><description><value>We all know the internet can be a scary place. In this talk I’ll focus on two ways I’ve found it positively terrifying. First, digging into tracking technologies, I have learned about the breadth and depth of ways our online activity is monitored, stored, and repackaged. Second, when starting out to learn a new skill, the tidal wave of information available online can be overwhelming.</value><value>Using the PyData stack to explore and visualize different data sources, including a new dataset from Mozilla, we’ll examine some of the many types of online tracking. My goal is to leave the audience with:</value><value>1) A sense of the breadth of tools in the PyData toolbox that can be applied to a real-world analysis
  2) An understanding of a few methods of online tracking so they can be more informed internet citizens</value><value>In particular, now that the EU’s General Data Protection Regulation (GDPR) has come into force, we can explore the data in light of EU citizens’ new rights, and the new responsibilities of companies worldwide.</value><value>Along the way, I’ll also talk about becoming a software engineer, then a builder of data science tools, and my new journey into data science. Being self-taught can be, lonely, scary, and full of embarrassing pitfalls. I’ll share some stories about my learning journey, and the people and resources that have supported me.</value></description><date><value>Thursday 26 July</value></date><tags><value>Visualization</value><value>Data Science</value><value>Data</value><value>Web Crawling</value><value>Privacy</value></tags></item><item><title><value>The rise of Python in the data communities</value></title><author><value>Alexys Jacob</value></author><description><value>A retrospective and prospective of Python’s adoption in the </value><value>data-driven industries</value><value> and how it has and should influence its ecosystem and communities.</value><value>Thanks to its versatility, Python’s usage and adoption has changed a lot over the last decade to go beyond the very act of software programming.</value><value>From Developers to SysOps, closely followed by Scientists and Data analysts, Python has spread to become a common tongue for a wide range of people.</value><value>We will start by looking at how this increased adoption impacted Python ecosystem and is still shaping it today. While </value><value>this talk is not walk through all the Python technologies around data</value><value>, some of them will be outlined so you will hear words like Numpy, Pandas or Jupyter.</value><value>Then we will try to project ourselves in the future and by </value><value>highlighting the pitfalls Python has to overcome</value><value> to keep up with its pace and mature in its ability to </value><value>scale</value><value>!</value><value>Draft of the agenda</value></description><date><value>Thursday 26 July</value></date><tags><value>Big Data</value><value>Engineering</value><value>Data Science</value><value>Community</value><value>DevOps general</value></tags></item><item><title><value>The naïve programmer</value></title><author><value>Daniele Procida</value></author><description><value>Since Picasso encountered Henri Rousseau over a century ago, modern art has been acutely aware of the value of the contributions that the naïve artist can make. Art negotiates the relationship between sophistication and naïvety with care and intelligence.</value><value>What does our programming culture make of the naïve programmer?  What can programming gain or learn from this encounter? </value><value>The naïve programmer is simply an unsophisticated programmer. Nobody is born sophisticated. Even the most sophisticated programmers were once naïve programmers. What’s more, the adoption of programming simply as a tool to solve immediate problems, by ever more people without any formal training, means that the number of naïve programmers will become greater, not less. Are we ready for this?</value><value>This talk will explore the relationship, illuminating it with examples from the world of painting, art and music, and will offer some lessons that our own industry and culture should be ready to learn from.</value><value>(This talk will already have been presented as a keynote at DjangoCon Europe 2018)</value></description><date><value>Wednesday 25 July</value></date><tags><value>Beginners</value><value>Best Practice</value><value>Education</value><value>Learning</value></tags></item><item><title><value>Trio: A pythonic way to do async programming</value></title><author><value>Emmanuel Leblond</value></author><description><value>Concurrent programs are super useful: think of web apps juggling lots
of simultaneous downloads and websocket connections, chat bots
tracking multiple concurrent conversations, or web spiders fetching
pages in parallel. But writing concurrent programs is complicated,
intimidating to newcomers, and often challenging even for experts.</value><value>Does it have to be? Python is famous for being simple and
straightforward; can Python make concurrent programming simple and
straightforward too?
Trio is an attempt to address this question by the positive !</value><value>By taking advantage of new Python 3 features (async/await keywords,
async loops and context managers etc.) while dropping legacy concepts that
older asynchronous frameworks has to maintain, Trio defines a new set of
primitives that make it dramatically easier to write correct concurrent programs.</value><value>In this talk, we will describe those primitives, and demonstrate how to
use them to implement a basic algorithm for speeding up TCP connections.
Compared to the best previous Python implementation, our version turns out to
be easier to understand, more correct, and dramatically shorter.</value></description><date><value>Wednesday 25 July</value></date><tags><value>Open-Source</value><value>Programming</value><value>ASYNC / Concurreny</value><value>Architecture</value></tags></item><item><title><value>The Challenges of Maintaining a Popular Open-Source Project</value></title><author><value>Raphael Pierzina</value></author><description><value>In this talk, I will give an insight into what it means to maintain a popular project for me personally, what it involves and what we as a community can do to help out and finally why I think it’s an important discussion to have.</value><value>Cookiecutter is a command-line utility that creates projects from templates. It is free and open-source software distributed under the terms of a permissive BSD-3 license. With around 180 individual contributors, more than 1000 public templates on GitHub alone, and multiple talks at conferences, it is fair to say that there is a small community around it.</value><value>But who are the people behind the project and what is it that they are doing?</value><value>It’s been three years since I was granted the commit bit by the core team. I have learned a lot about FOSS communities and also about myself. At times I struggle with balancing my day job as a full-time Software Engineer and maintaining Cookiecutter and other FOSS projects in my spare time. By now I’m OK with not responding to issues immediately and closing pull requests. However it took me quite a while to get to this point.</value><value>Maintaining FOSS projects can be incredibly rewarding and fun, but it can also be quite frustrating. It involves so much more than writing code or merging PRs, and yet sometimes it feels like that’s what most people think. The goal of this talk is to start a conversation around this topic and hear what other EuroPython attendees think about it and their challenges as contributors or maintainers.</value></description><date><value>Friday 27 July</value></date><tags><value>Open-Source</value><value>Community</value></tags></item><item><title><value>System testing with Pytest, Docker, and Flask</value></title><author><value>Neil Gall</value></author><description><value>The composability of fixtures in pytest is an improvement over traditional
xUnit setup/teardown, reducing the incentive to commit testing crimes such as
multi-stage and stepwise tests. This is great out of the box for unit tests,
but I’m going to show how to combine the power of pytest fixtures with Docker
to build high-level integration tests for microservices or other complex
systems with multiple components. I’ll then build on that to show how to embed
mock web services written with Flask right into the test code.</value><value>With a sample Java application that makes use of some external resources to
offer a data processing service I’ll first quick an overview of Pytest, Docker,
and Flask. Then I’ll mix some pre-built code with live test coding to
demonstrate how to build high-level system tests which spin up the application
and its dependencies in Docker. I’ll then mock one of the external dependencies
using Flask, allowing the test to control and verify interaction between the
system components. Finally I’ll show how to wrap the Flask application in a
WSGI middleware that lets the test inspect interaction with the mocked service.</value><value>From a learning and development point of view, building your own is better than
re-using someone else’s code so I’ll show how the support code for these
features is relatively simple and how the audience can build it themselves to
exactly meet their own needs. And I’ll do it all with a sense of fun, a joke
or two and maybe a little storytelling.</value></description><date><value>Friday 27 July</value></date><tags><value>python</value><value>Docker</value><value>Test Libraries (pyTest/node/...)</value><value>Web Servers and MicroFWs (Flask/Tornado/Nginx/...)</value></tags></item><item><title><value>The Boring Python Office Talk - Automate Powerpoint, Excel, and PDF with Python</value></title><author><value>Stefan Baerisch</value></author><description><value>We will have a quick tour of the many ways Python gives us to handle DOCX, XLSX, PPTX, and PDF and automate some boring office tasks.</value><value>Many things are more interesting than office file formats like DOCX, XLSX, PPTX, and PDF. Still, while working with office formats does not seem to be the most fun, it is useful. But we can do better than just useful. With the Python and some great libraries, it is possible to have Python do much of the work you would have to do otherwise:</value><value>In this talk, we will have a look at a usual working day for Bob and Ann, two fictional office works. Both Bob and Ann work office jobs, but while Bob does all of his work by hand, but Ann knows Python. We will look at different tasks that Bob wants to do, such as preparing an Excel report, building a Powerpoint presentation, or rearranging a PDF. Then, we will look how Ann use Python and some exciting libraries to automate these task.</value><value>During the talk, we will use Bob and Ann to consider different task related to office file formats. We will then look at the Python libraries that are available. Then, using this library, we will see how an otherwise boring task can be automated. 
The goal of the task is to showcase the libraries to Python offers to work with standard office formats and provide you with a starting point for your own office automation.</value><value>After this talk, you will know how to automate at least some of your daily office tasks. You may also be bored because Python is doing so much of your work for you.  If you know basic Python programming, you will be right at home. There will be some use of Pandas,  but it is not required.</value></description><date><value>Friday 27 July</value></date><tags><value>Python general</value><value>Python 3</value></tags></item><item><title><value>Succinct data structures for python</value></title><author><value>Konstantin Ignatov</value></author><description><value>This is a presentation of and call for participation in development and testing of Python bindings to Succinct Data Structure Library.</value><value>From Wikipedia: Succinct data structures can represent an object (such as a bitvector or a tree) in space close to the information-theoretic lower bound of the object while supporting operations of the original object efficiently. The theoretical time complexity of an operation performed on the classical data structure and the equivalent succinct data structure are (most of the time) identical.</value><value>Currently bindings are provided for:</value><value>Original library:  https://github.com/simongog/sdsl-lite
Most of examples from SDSL cheat sheet and SDSL tutorial are implemented.</value></description><date><value>Thursday 26 July</value></date><tags><value>Algorithms</value><value>Data</value><value>Science Track</value><value>Structures</value><value>C-Languages</value></tags></item><item><title><value> Standardize Testing in Python</value></title><author><value>Bernat Gabor</value></author><description><value>In this talk, Bernat will introduce tox, an open source tool with the bold vision of standardizing testing. From a CIs point of view, testing contains much more than just unit and/or integration tests; other things like code style checks, packaging, testing under various versions of Python, and checking that documentation still generates, are just as important. Things quickly start to spiral out of control once you add into the mix that there are many tools and ways to accomplish each of these tasks. In this talk, we will learn how tox tries to abstract away all this complexity, how to easily run CI tests on your local machine, and how one can use this tool inside your CI frameworks to ensure high quality and easily maintainable packages.</value></description><date><value>Thursday 26 July</value></date><tags><value>Best Practice</value><value>Agile</value><value>Testing</value><value>Python general</value><value>Open-Source</value></tags></item><item><title><value>SSLError, now what?</value></title><author><value>Christian Heimes</value></author><description><value>TLS/SSL is the most important and widely-used protocol for secure and encrypted communication, e.g. HTTPS. It offers more than just encryption. TLS also ensures data integrity and strong authentication with X.509 certificates. But it provides merely a false sense of security, if you use it wrong.</value><value>Have you ever encountered ssl.SSLError: [SSL: CERTIFICATE</value><value>VERIFY</value><value>FAILED], while connecting to a server, but you didn’t understand what is going on? Are you running production code without TLS/SSL protection or with certificate validation disabled, because you couldn’t figure out how to make it work correctly?</value><value>I’ll give you the rundown of the basic cryptographic building blocks, protocol handshake, inner structure of certificates, and PKI. You’ll learn about the best practices, debugging tools and tips how to diagnose TLS/SSL and how to deal with certificates.</value></description><date><value>Wednesday 25 July</value></date><tags><value>Security</value><value>Infrastructure</value><value>HTTP</value><value>failures/mistakes</value><value>Internet</value></tags></item><item><title><value>Type annotations with larger codebases</value></title><author><value>Stephan Jaensch</value></author><description><value>You’ve heard about type annotations, you know they help reduce bugs and improve documentation especially for large codebases, and you’ve attended an introductory talk or read a tutorial about using them. But how do you get started using them with your big, existing codebase? How do you make sure your colleagues will be annotating new code they write - or existing code they’re changing? And how do you get around some of the issues you might run into when using the still-beta type checker mypy on your codebase?</value><value>This talk will start where the typical introductory Python type annotation talks end and discuss the real-world challenges when starting to annotate types with an existing codebase of tens or hundreds of thousands of lines of code. I’ll walk you through best practices learned from doing just that at Yelp, telling you about some of the roadblocks we hit (and how we got past them).</value><value>We’ll also take a look at:
- how you can get the most out of type annotations even with non-annotated third-party libraries
- how to deal with code patterns that currently don’t always work well with annotations
- when the only way to get proper type checking is through refactoring your code.</value></description><date><value>Thursday 26 July</value></date><tags><value>Best Practice</value><value>Type-Hinting</value><value>Python 3</value></tags></item><item><title><value>reno: A New Way to Manage Release Notes</value></title><author><value>Doug Hellmann</value></author><description><value>reno is a tool for managing release notes in projects that support multiple branches of development, and releases, simultaneously. It solves the problem of managing release notes within patches that fix bugs, and makes it easier to cherry-pick changes between branches (allowing backports or forward ports). This talk will cover the requirements, and constraints, that led us to design and build reno. I will also show how to use it to create notes and publish them in your packages and via Sphinx-based documentation. Then I will talk about the impact reno, and the release notes publishing automation, had on our project and release processes.</value></description><date><value>Wednesday 25 July</value></date><tags><value>Tooling</value><value>Open-Source</value></tags></item><item><title><value>Rust and Python - Oxidize Your Snake</value></title><author><value>Sven-Hendrik Haase</value></author><description><value>Rust is a safe and modern systems programming language that is easily able to call and be called from Python. As such, it is a prime candidate for replacing C for writing Python modules that have to be fast or that have to interact with other native code. Rust is extremely fast and makes it very hard to get concurrency wrong.</value><value>Many ways of making Python call into lower level have appeared over the years such as CFFI, ctypes, boost.python, cython, SWIG. All of them are cumbersome in their own ways. PyO3 is a Rust library that makes it easy and simple to write native Python modules with minimal glue code and no crazy tooling required. It even works cross-platform without problems.</value><value>The talk shows some sample code of PyO3-based modules and compares it with the code of the alternatives as well as the alternative’s cross-platform support and tooling.</value><value>The goal is for the audience to be informed about a new safe and modern way of writing native Python modules.</value><value>The audience doesn’t need any prerequisites except for a healthy interest in native code and Python modules. C knowledge is optional.</value></description><date><value>Friday 27 July</value></date><tags><value>Performance</value><value>Python general</value><value>All Other Programming Languages</value><value>C-Languages</value><value>Compiler and Interpreters</value></tags></item><item><title><value>Reliability in distributed systems</value></title><author><value>Jiri Benes</value></author><description><value>Is your system stable? Do you know what happens if any of your system’s dependency will start failing? Do you even know what exactly each part of your system does or did any time in the past? Or how fast you will identify root of the problem in case your system goes down at 2am?</value><value>The talk focuses on distributed systems (microservices, APIs that communicate with databases, memory, third party services, etc.), monitoring, their failures and recovery in order to help you answer yourself questions above.</value><value>First part aims on importance of monitoring such systems on several levels - monitoring of hardware, application monitoring, monitoring from outside of the systems, detecting malfunctions based on anomalies within system’s data flows.</value><value>Second part presents several standard techniques for preventing system failure in case of outage of dependency and technique how to recover from inconsistent state after outage.</value><value>Content of presentation is helpful and interesting for beginners and intermediates. Senior developers and developers working on reliable distributed systems should bear in mind content of this presentation and master shown techniques.</value></description><date><value>Wednesday 25 July</value></date><tags><value>Best Practice</value><value>Microservices</value><value>Engineering</value><value>Architecture</value><value>failures/mistakes</value></tags></item><item><title><value>Recursion, Fractals, and the Python Turtle Module</value></title><author><value>Hayley Denbraver</value></author><description><value>To understand recursion you must first understand recursion. Alternatively, you could come to a talk that will demonstrate the basics of recursion, using fractals drawn by the python turtle module! </value><value>What are fractals? Fractals are psychedelic designs that appear basically the same regardless of scale. They also appear all over nature–consider how a tree trunk and branches is similar in structure to branches and twigs or how a wave is made of mini waves. Best of all, fractals can be constructed using recursive functions. </value><value>What is the turtle module? It is part of the standard python library that can be used to draw amazing things and is commonly used as a tool for learning to code. </value><value>Come for the recursion, stay for the amazing fractals, and leave knowing that no turtles were harmed in the making of this talk.</value><value>This talk is suitable for all python enthusiasts. Those who love mathematics will love this talk. Advanced developers will learn more about a tool that can be used to teach people to code. There is something for everyone. </value></description><date><value>Wednesday 25 July</value></date><tags><value>Beginners</value><value>Fun and Humor</value><value>Education</value><value>Python general</value></tags></item><item><title><value>Rehabilitating Pickle</value></title><author><value>Alex Willmer</value></author><description><value>Pickle is a compact serialisation protocol for Python objects.  It could be a convenient way for Python programs and distributed systems to communicate. Unfortunately pickle is widely considered to be unsafe, and it has lead to several vulnerabilities over the years. As the Python manual warns</value><value>The pickle module is not secure against erroneous or maliciously 
  constructed data. Never unpickle data received from an untrusted
  or unauthenticated source.</value><value>Does this have to be the case? Can we use Pickle safely?</value><value>This talk will be a deep dive into what an attacker can do with a maliciously constructed pickles. I’ll show what defences you can implement against the common attacks, especially those that gain arbitrary code execution. I will present new research into other attacks, and mitigations. Finally I will review a few less known alternatives to pickle.</value><value>This talk will expand on a </value><value>PyLondinium 2018 lightning talk</value></description><date><value>Friday 27 July</value></date><tags><value>Configuration Management (Ansible/Fabric/Chef/...)</value><value>Security</value><value>Python general</value><value>Messaging and Job Queues (RabbitMQ/Redis/...)</value><value>Distributed Systems</value></tags></item><item><title><value>Recipe for text analysis in social media:</value></title><author><value>Eulalia Veny</value></author><description><value>The analysis of text data in social media is gaining more and more importance every day. The need for companies to know what people think and want is key to invest money in providing customers what they want. The first approach to text analysis was mainly statistical, but adding linguistic information has been proven to work well for improving the results.</value><value>One of the problems that you need to address when analyzing social media is time. People are constantly exchanging information, users write comments every day about what they think of a product, what they do or the places they visit. It is difficult to keep track of everything that happens. Moreover, information is sometimes expressed in short sentences, keywords, or isolated ideas, such as in Tweets. Language is usually unstructured because it is composed of isolated ideas, or without context.</value><value>I will talk about the problem of text analysis in social media. I will also explain briefly Naïve Bayes classifiers, and how you can easily take advantage of them to analyse sentiment in social media, and I will use an example to show how linguistic information can help improve the results. I will also evaluate the pros and cons of supervised vs unsupervised learning.</value><value>Finally, I will introduce opinion lexicons, both dictionary based and corpus-based, and how lexicons can be used in semi-supervised learning and supervised learning. If I have time left, I will explain about other use cases of text analysis.</value></description><date><value>Wednesday 25 July</value></date><tags><value>Case Study</value><value>Notebook</value><value>Natural Language Processing</value></tags></item><item><title><value>Pythonic code vs. performance</value></title><author><value>Łukasz Kąkol</value></author><description><value>Idiomatic Python is beautiful. If you’re new to Python, this talk is for you because I’m going to reveal the charm of python in front of you. I’ll present how boilerplate code can be replaced with idiomatic python. If you’re experienced python developer, this talk is also for you because I’ll compare the performance of the idiomatic code, both from CPU and memory point of view. Some of these results may surprise you.</value></description><date><value>Wednesday 25 July</value></date><tags><value>python</value><value>Performance</value><value>Best Practice</value><value>Programming</value><value>failures/mistakes</value></tags></item><item><title><value>Quantum Computing: a Very Gentle Glimpse into a Possible Future</value></title><author><value>Nick Radcliffe</value></author><description><value>This talk will aim to demystify quantum computing and will assume no prior knowledge. The speaker, while having studied Quantum Field Theory many years ago, has only relatively recently started learning properly about  Quantum Computing, and is going to try to take advantage of the fact that the ideas are relatively new to him to explain the key concepts simply, to an audience (probably) consisting mostly of non-experts in quantum computing.</value><value>Key questions I will try to cover are likely to include
  * What is quantum mechanics anyway?
  * What are quantum computers? Are they real? And are they really faster/more powerful than classical computers?
  * What is a qubit (the quantum computing analogue of a bit)
  * What problems are likely to be solved well by quantum computers?
  * How do you program a quantum computer?
  * How do you get results out of a quantum computer? What kind of results are they anyway? Do you have to visit other universes to get them?
  * Can you get access to quantum computers online and use Python to program them (kind-of!)
  * Will quantum computers break all encryption?
  * What are Shor’s and Grover’s algorithms and (just possibly) how do they work?
  * What are the challenges with building practical quantum computers?
  * What is the Python story with Quantum Computing?</value></description><date><value>Wednesday 25 July</value></date><tags><value>Other Hardware</value><value>Universe</value><value>Mind Bending</value><value>Physics</value><value>Cryptography</value></tags></item><item><title><value>Quart a asyncio alternative to Flask</value></title><author><value>Philip Jones</value></author><description><value>Flask is a great web mirco-framework, that is best utilised with
event-loop concurrency. Sadly with Flask the event-loop framework
can’t be asyncio, although some extensions (Flask-Aiohttp) have
tried. Quart is the solution as it shares the Flask API and is
based on asyncio. In addition Quart goes beyond Flask adding
HTTP/2 and websockets.</value><value>This talk will outline why event-loop concurrency is a good
choice for web servers, why asyncio is a good choice and then
give an overview of Quart, demonstrating features that go beyond
the Flask framework.</value></description><date><value>Friday 27 July</value></date><tags><value>HTTP</value></tags></item><item><title><value>Python on Windows is Okay, Actually</value></title><author><value>Steve Dower</value></author><description><value>Packages that won’t install, encodings that don’t work, installers that ask too many questions, and having to own a PC are all great reasons to just ignore Windows. Or they would be, if they were true.
Despite community perception, more than half of Python usage still happens on Windows, including web development, system administration, and data science, just like on Linux and Mac. And for the most part, Python works the same regardless of what operating system you happen to be using. Still, many library developers will unnecessarily exclude half of their potential audience by not even attempting to be compatible.
This session will walk through the things to be aware of when creating cross-platform libraries. From simple things like using pathlib rather than bytestrings, through to all the ways you can get builds and tests running on Windows for free, by the end of this session you will have a checklist of easy tasks for your project that will really enable the whole Python world to benefit from your work.</value></description><date><value>Wednesday 25 July</value></date><tags><value>Windows</value><value>Best Practice</value><value>Cross-Platform-Development</value><value>CPython</value></tags></item><item><title><value>Python in scientific computing: what works and what doesn't</value></title><author><value>Michele Simionato</value></author><description><value>There is no want of technologies for doing scientific calculations in Python. In this talk I will share some hard-learned knowledge about what works and what doesn’t with the libraries we are using at GEM (the Global Earthquake Model foundation).
I will show how the following libraries fare with respect to our main concerns of performance, simplicity, reliability and portability</value><value>and I will talk about several library bugs we found and had to work around. I will also talk about some libraries that we do not use (such as cython, numba, dask, pytables, …) and the reason why we do not use them.
Hopefully this will be useful to people using or planning to use a similar software stack.</value><value>My slides are here: https://gitpitch.com/micheles/papers/europython2018</value></description><date><value>Wednesday 25 July</value></date><tags><value>Science</value><value>Scientific Libraries (Numpy/Pandas/SciKit/...)</value><value>Case Study</value></tags></item><item><title><value>Python, Docker, Kubernetes, and beyond ?</value></title><author><value>Peter Babics</value></author><description><value>Have you ever tried to manage deployment of multiple python applications through various
linux distributions ? If so, you must have heard of Docker and maybe also Kubernetes.
Distributing python applications using docker is simple and allows to create static packages
containing everything required for them to run. Also it allows to freeze everything, packages, available
libraries, files on filesystem.
In my speech I would like to tell you about our brief journey, of moving our trading platform
from standalone application directly on host system, through deploying it in docker and latter
moving it to kubernetes. I will explain our struggles with implementing stable and fast CI using Gitlab CI and Docker,
image (package) storage and cleanup of old images and finally I will tell you how we are deploying
our platform to kubernetes, with nothing more than yaml-s and templating.</value></description><date><value>Wednesday 25 July</value></date><tags><value>Docker</value><value>Infrastructure</value><value>Deployment/Continuous Integration and Delivery</value></tags></item><item><title><value>Python Decorators: Gift or Poison?</value></title><author><value>Anastasiia Tymoshchuk</value></author><description><value>Why would you ever need to use decorators in Python? </value><value>
Have you ever had the task when you need to use one function in few places and you really wanted to avoid of code duplicating? For example to add some logging into functions or timers, etc. Decorators in Python are super powerful with these tasks, but at the same time they are super complicated, sometimes even magical. When I started learning Python, Decorators were really like a magic: how to use them, how are they working, lots of questions. The goal is to make the things easier and clear to answer a question: to use or not to use Decorators in your project. </value><value>
What’s in the Talk: </value><value>
- Functions nature in Python </value><value>
- Magic of a Decorator </value><value>
- Basics </value><value>
- When to use Decorators </value><value>
- Examples </value><value>
- Even more Python magic</value><value>My slides are here: https://atymo.me/projects/presentations/GiftOrPoison/ </value><value>
Code examples: https://github.com/atymoshchuk/python_tutorials</value></description><date><value>Wednesday 25 July</value></date><tags><value>Best Practice</value><value>Python general</value><value>Clean Code</value><value>failures/mistakes</value><value>Use Case</value></tags></item><item><title><value>Python 3: ten years later</value></title><author><value>Victor Stinner</value></author><description><value>Draft of the talk:</value><value>Growing populary of the Python programming language</value><value>Port Python 2 code to Python 3</value><value>Port Python 2 code to Python 3</value><value>No: “Add support for Python 3”, don’t loose Python 2 support</value><value>Port Python 3 code to Python 2</value><value>Python changes to make the transition smoother:</value><value>PEP 414: u"syntax” reintroduced in Python 3.3</value><value>Analysis of the transition</value><value>Bugs that won’t be fixed in Python 2 anymore</value><value>Performance</value><value>Evolutions of the Python language</value><value>Python 3.5</value><value>PEP 492: async/await “keywords” for asyncio.
(Really keywords in Python 3.7.)</value><value>PEP 448: Generalized unpacking:
</value><value>head, *tail = list</value><value>
</value><value>mylist = [1, 2, **other_list]</value><value>
</value><value>mydict = {"key": "value", **other_dict}</value><value>Python 3.6</value><value>PEP 515: </value><value>million = 1_000_000</value><value>Bury Python 2?</value></description><date><value>Friday 27 July</value></date><tags><value>Python 2</value><value>Python 3</value><value>CPython</value></tags></item><item><title><value>Python and GraphQL</value></title><author><value>Alec MacQueen</value></author><description><value>GraphQL query language is gaining popularity and seeing more adoption.
This is mainly due to the efficiency with which consumers can get their data, the ease with which you can document and explore your API and the powerful tooling that has been built around the query language.</value><value>This talk is for novices and experts of GraphQL alike and aims to cover the basics of the query language, how to implement it using Flask and SQLAlchemy and to take a deeper dive into how Python’s type-hinting can be used to generate your GraphQL schema. </value><value>I’ll also be talking about some of the tooling that can be used to provide consumers and developers working on your API with a great development experience. Next I’ll cover some ways you can use these tools to empower your development process and get you developing ‘API First’. </value><value>To get the most out of this talk you should have a general understanding of APIs, Python frameworks and Python ORMs. However, my aim is to make it as accessible as possible for developers of all experience levels.</value></description><date><value>Friday 27 July</value></date><tags><value>Web</value><value>Python 3</value><value>APIs</value></tags></item><item><title><value>Python and Web Sockets</value></title><author><value>Anton Caceres</value></author><description><value>Modern web-apps require bi-directional communication, reacting not only to user actions but also to server events. This can be achieved elegantly using Web Sockets, a protocol standardized by W3C to be a default tool for full-duplex connections on the Web.</value><value>Although most web-frameworks do not support Web Socket integration out of the box, Python has multiple options available making it a piece of cake. This talk showcases integrating Web Sockets in Python web-apps, focusing on situations that can make good use of it and presenting implementations with most popular frameworks: Tornado and Django. We will explore a similar style but different underlying technologies of both, and finish with a live demo.</value></description><date><value>Thursday 26 July</value></date><tags><value>Web</value><value>Web Track</value><value>Web Servers and MicroFWs (Flask/Tornado/Nginx/...)</value><value>ASYNC / Concurreny</value><value>Django</value></tags></item><item><title><value>Python 2 is dead! Drag your old code into the modern age.</value></title><author><value>Becky Smith</value></author><description><value>The clock is ticking on Python 2.7, with support to be dropped in January 2020.  With major dependencies such as Django, NumPy and pandas moving to Python 3 only, the time has come for even big established codebases to consider upgrading.  Many organisations are still postponing for various reasons; we will attempt to demonstrate that with a bit of planning and perseverance, and the assistance of some handy tools, we can embrace the future!</value><value>This session will provide a first-hand perspective on how we upgraded a large (~65,000 lines of python code) 8-year-old Django project with multiple external dependencies from Python 2.7 to Python 3.6.  </value><value>We will briefly discuss the benefits of upgrading to Python 3, and architectural considerations.  The session will primarily focus on the practicalities of upgrading the code itself.  We will not try to provide a single “best” solution for upgrading to Python 3, but rather will introduce some of the available tools, provide an insight into how we used them, and their advantages and disadvantages from our experience.  We will discuss preparatory steps and approaches, strategies for dealing with external dependencies, and “gotchas” that we encountered during the process.</value><value>The aim of this session is to provide an example of how a Python 3 upgrade on an established commercial product can be successfully completed, and to furnish audience members with a set of tools and strategies to help them with their own projects.  </value><value>Prerequisites: basic knowledge of Python. </value></description><date><value>Wednesday 25 July</value></date><tags><value>Case Study</value><value>GEO and GIS</value><value>Python 3</value><value>Django</value></tags></item><item><title><value>PyPI: Past, Present and Future</value></title><author><value>Nicole Harris</value></author><description><value>The Python Package Index (PyPI) is the principal repository of software packages for the Python programming language.</value><value>In May 2018, PyPI served 12.3 billion HTTP requests, with 1.4 million people visiting pypi.org via their web browser.</value><value>The Python community depends on PyPI for the ongoing functioning of the entire Python ecosystem.</value></description><date><value>Thursday 26 July</value></date><tags><value>PyPi</value></tags></item><item><title><value>Proper Django Testing</value></title><author><value>Martin Angelov</value></author><description><value>I’m currently working in a Django project, We have a lot of tests (~ 2000) and a fair amount of code coverage currently. The system and the requirements of the client grows everyday (like in every other project these days). I’ve convinced that the effort we put in making each of these tests pays off daily.</value><value>Speaking with other Django and Python developers I’ve noticed that people often “overengineer” their unit tests and they usually tend to break their principles.</value><value>In this presentation I will talk mainly about unit tests. - what packages we use and how and when to use them (properly). We will take a look on how we structure our projects and how this structure actually helps us write proper unit tests. I will start with the very basic tests and continue with the mocking technique. In the end I’m going to refer to other testing methods (integration, E2E, validation testing,  visual regression, etc.).</value><value>The goal of this talk is to make people better in testing and to show them how these skills will fasten the development process and  help them maintain their project and it’s quality.</value></description><date><value>Thursday 26 July</value></date><tags><value>Best Practice</value><value>Django</value><value>clients</value><value>Testing</value><value>APIs</value></tags></item><item><title><value>Programming paradigms for physical computing and IoT</value></title><author><value>Ben Nuttall</value></author><description><value>A look at the GPIO Zero library for Raspberry Pi yields the blueprint for a Pythonic API for programming the behaviour of interconnected devices.</value><value>GPIO Zero provides a multi-paradigm programming interface to GPIO devices:
- procedural (polling)
- procedural (blocking)
- event-driven (callbacks)
- declarative</value><value>Start with simple scripts to control LEDs and buttons on a breadboard, learn to prototype ideas and move on to declaring interaction between more advanced devices in the home and beyond.</value><value>These options for device programming, along with the extensibility of the library, provide the means to program complex behaviour using simple Pythonic code. </value></description><date><value>Friday 27 July</value></date><tags><value>Gadgets</value><value>Internet of Things (IoT)</value><value>Sensors</value><value>Raspberry PI</value><value>Hardware/IoT Track</value></tags></item><item><title><value>Productionizing your ML code seamlessly</value></title><author><value>Lauris Jullien</value></author><description><value>Data science and Machine Learning are hot topics right now for Software Engineers and beyond. And there are a lot of python tools that allow you to hack together a notebook to quickly get insight on your data, or train a model to predict, or classify. Or you might have inherited some data wrangling and modeling {Jupyter/Zeppelin} notebook code from someone else, like the resident data scientist. </value><value>The code works on test data, when you run the cells in the right order (skipping cell 22), and you believe that the insight gained from this work would be a valuable game changer. But now how do you take this experimental code into production, and keep it up-to-date with a regular retraining schedule? And what do you need to do after that, to ensure that it remains reliable and brings value in the long term?</value><value>These will be the questions this talk will answer, focusing on 2 main themes:
     1. What does running an ML model in production involve?
     2. How to improve your development workflow to make the path to production easier?</value><value>This talk will draw examples from real projects at Yelp, like migrating a pandas/sklearn classification project into production with pyspark, while aiming to give advice that is not dependent on specific frameworks, or tools, and is useful for listeners from all backgrounds.</value></description><date><value>Friday 27 July</value></date><tags><value>Best Practice</value><value>Scientific Libraries (Numpy/Pandas/SciKit/...)</value><value>Data Science</value><value>Notebook</value><value>Machine-Learning</value></tags></item><item><title><value>Processing Geodata using Python and Open Source Modules</value></title><author><value>Martin Christen</value></author><description><value>The need for processing small-scale to large-scale spatial data is huge. In this talk, it is shown how to analyze, manipulate and visualize geospatial data by using Python and various open source modules.</value><value>The following modules will be covered:</value><value>Github repo: https://github.com/martinchristen/EP2018_Geo</value></description><date><value>Friday 27 July</value></date><tags><value>Visualization</value><value>Data</value><value>Big Data</value><value>Jupyter/iPython</value><value>Science</value></tags></item><item><title><value>My Story with Python and Open Source</value></title><author><value>Nicola Iarocci</value></author><description><value>This is the story of how I (and with me, my company) went from lonely, introvert, C# developer to open-source Python author and maintainer, speaker, trainer, consultant, and all-around community junkie. With some luck, in the process, you will also hear a few hints on how to become a good open source contributor and have a chance to ponder on the pros and cons (yes there are cons too) of going full monty with open source. We will also look at what changed in my company as we slowly switched from closed, in-house eco-systems to the open source field.</value></description><date><value>Wednesday 25 July</value></date><tags><value>python</value><value>Open-Source</value></tags></item><item><title><value>PEP 557* versus the world</value></title><author><value>Guillaume Gelin</value></author><description><value>Python 3.7 will ship with a new module called </value><value>dataclasses</value><value>, which has been defined in PEP 557.</value><value>This talk will deeply cover </value><value>dataclasses</value><value>, but also </value><value>attrs</value><value>, </value><value>box</value><value>, </value><value>thingy</value><value>, and others.</value><value>Post-conference notes:
- Slides: https://slides.com/ramnes/pep-557
- Implementations and micro-benchmark: https://github.com/ramnes/pep-557-vs-the-world</value></description><date><value>Thursday 26 July</value></date><tags><value>Software Design</value><value>Data Structures</value><value>Clean Code</value><value>Python 3</value><value>Developing with Python Track</value></tags></item><item><title><value>More Than You Ever Wanted To Know About Python Functions</value></title><author><value>Mark Smith</value></author><description><value>What exactly </value><value>are</value><value> functions? Let’s talk about functions, methods, callables and closures - what they are, what you can give them, what they can give you, what you can do with them … and what’s inside.</value><value>You probably think you already know everything about functions, but you probably don’t!</value><value>Input &amp; Output</value><value>: How do you get things in and out of functions? I’ll cover parameters and the myriad of ways they can be specified, provided and accessed - including helpful hints to avoid common mistakes! I’ll cover return values, briefly, along with variable scopes and exceptions.</value><value>Closures</value><value>: What are they, how do they work and how they can affect memory usage.</value><value>Methods</value><value>: How does a method differ from a function, when are they made, how do they work (where does </value><value>self</value><value> come from?) and how to access the function inside every method.</value><value>__magic__</value><value>:Make your own callables from any object!</value><value>Introspection</value><value>: Using modern Python techniques, what can you find out about a function, and what can you do with that information?</value><value>Bytecode</value><value>: What happens if you open up a function and look at its insides? Can you change it and put it back together again? (Spoiler: Yes, you can.)</value><value>By the end of this talk, I guarantee* you’ll know more about callables than when you walked in, along with techniques both practical and so extreme your colleagues will never let you merge them to master.</value><value>(*This guarantee is legally non-binding and cannot be redeemed in any way.)</value></description><date><value>Thursday 26 July</value></date><tags><value>Type-Hinting</value><value>Mind Bending</value><value>Programming</value><value>Functional Programming</value></tags></item><item><title><value>Postgres at any scale</value></title><author><value>Craig Kerstiens</value></author><description><value>We’ll start with the basics you need to know as an app developer about interacting with your database, then dig into how you can start to analyze performance. We’ll look at things you need to know for a small application, then the things you should be cautious of as you start to layer in other items you need to be aware of for performance including:</value></description><date><value>Wednesday 25 July</value></date><tags><value>PostgreSQL</value><value>Databases</value></tags></item><item><title><value>Mocks, fakes, dummies, stubs and spies: Successfully isolating the snake</value></title><author><value>Mario Corchero</value></author><description><value>Ever wonder what mocks, fakes, dummies stubs or spies are? Or what the differences are between them and when one should be used instead of another? We will take a deep look into these concepts, what they mean and examples on how to use/create them in Python.</value><value>In this talk, we will quickly explore the reasons for using mock and how it works, quickly jumping into the different concepts of testing doubles and how they can be used in Python. The talk will include as part of the examples from some features in mock coming in Python 3.7 that might change how usual mocking is performed. The talk builds on the writings of Gerard Meszaros and Martin Fowler about testing doubles, focusing on how to apply them to Python.</value><value>This session, which will review test isolation concepts and the unittest.mock module, is structured in a way that both beginners and intermediate developers will learn from it. A basic knowledge of testing is recommended. Intermediate developers will leave the room with a clear understanding of the tools - further than just using simple mocks - to successfully fake dependencies. Multiple “not so well known” features of unittest.mock will be presented so we can shape those objects to behave functionally different.
Unittest.mock is an extremely useful library which is commonly underused, this talk aims to bring clarity into stubbing in general and into medium/advanced mock features to ease and remove pain when users work with it.</value><value>Outline: </value><value>• Intro (1’)
• Why doubles are important (1’)
• Main qualities of a double (2’)
• Overview of unittest.mock (5’)
• Testing doubles (15’)
    o   Doubles (1’)
    o   Stubs (2’)
    o   Spies (3’)
    o   Mocks (5’)
    o   Fakes (2’)
• Wrap up, conclusion and pointers (2’)
• Q&amp;A (Expected 5’)</value></description><date><value>Thursday 26 July</value></date><tags><value>python</value><value>Testing</value><value>Python general</value></tags></item><item><title><value>Microservices and Serverless in Python projects</value></title><author><value>Jose Manuel Ortega</value></author><description><value>Monoliths, microservices and now Serverless. Function as a Service (FaaS) platforms give us new ways to attack old problems. The possibility of executing functions as a service allows designing scalable and highly parallel applications, but on the other hand, this kind of applications require a particular programming style. For example, bundling dependencies and managing state is not trivial.</value><value>However, there are plenty of tools and frameworks to help you code serverless applications with Python, and once you get started it is not complicated.</value><value>In this talk we will mention the advantages of Serverless and we will focus on the situations in which we can introduce it into our Python projects. We will use AWS Lambda for the examples.</value><value>These could be the main talking points:</value><value>Introducing Serverless and Function as a Service (FaaS) in Python projects
Advantages of Microservices and Serverless 
AWS Lambda functions with chalice
Testing AWS lambda with docker</value></description><date><value>Thursday 26 July</value></date><tags><value>Public Cloud (AWS/Google/...)</value><value>Microservices</value><value>Docker</value><value>Architecture</value><value>DevOps general</value></tags></item><item><title><value>May the Fuzz be with you</value></title><author><value>Heidi Thorpe</value></author><description><value>This talk will explore how Long short-term memory (LSTM) recurrent neural networks combined with Python can use Artificial Intelligence to provide invalid, unexpected, or random data as inputs for testing software and finding network security vulnerabilities. The technique of machine generated test inputs is an established testing approach and is sometimes referred to as fuzzing. I will take this idea further by presenting how various types of machine learning can provide novel outlier test cases to harden software and networks in a way that is superior to using only randomness. Instead, I will discuss how to train neural networks to produce better fuzzing data.This is a natural progression from my work with machine learning and image processing using support vector machines (SVM) and Generative Adversarial Networks (GAN) which I have applied to this new domain of software testing.
This a beginner level talk aimed at all software developers and testers with a goal to giving attendees a basic understanding of fuzz testing.</value><value>Timing of presentation
1 Introduction 0-5 minutes 
2 What is a neural fuzzing 5-10 minutes 
3 How does fuzzing work 10-15 minutes 
4 Steps to train a deep neural network 15-20 minutes 
5 Implementing a basic LSTM neural network for security testing 20-25 minutes 
6 Conclusion and Questions 25-30 minutes</value></description><date><value>Thursday 26 July</value></date><tags><value>python</value><value>Security</value><value>Testing</value><value>Programming</value><value>Data Science</value></tags></item><item><title><value>Marge: A bot for better Git'ing</value></title><author><value>Mika Boström</value></author><description><value>Over the past decade, development workflows across industries have converged towards Continuous Integration (CI) with pull requests (PR). Automated testing and artifact generation is useful, but the logic is often flawed: test are run against the source branch, and side effects from successful merges are not accounted for. Marge-bot improves this workflow by allowing to test the PR against the target branch while providing a host of other conveniences.</value></description><date><value>Thursday 26 July</value></date><tags><value>Best Practice</value><value>Development</value><value>Deployment/Continuous Integration and Delivery</value><value>Git</value><value>Tooling</value></tags></item><item><title><value>Lies, damned lies, and statistics</value></title><author><value>Marco Bonzanini</value></author><description><value>Statistics show that eating ice cream causes death by drowning.</value><value>If this sounds baffling, this talk will help you to understand correlation, bias, statistical significance and other statistical techniques that are commonly (mis)used to support an argument that leads, by accident or on purpose, to drawing the wrong conclusions.</value><value>The casual observer is exposed to the use of statistics and probability in everyday life, but it is extremely easy to fall victim of a statistical fallacy, even for professional users.</value><value>The purpose of this talk is to help the audience understand how to recognise and avoid these fallacies, by combining an introduction to statistics with examples of lies and damned lies, in a way that is approachable for beginners. </value><value>Agenda:</value></description><date><value>Friday 27 July</value></date><tags><value>Beginners</value><value>Education</value></tags></item><item><title><value>Let’s Build a Python Profiler in 25 LOC</value></title><author><value>Noam Elfanbaum</value></author><description><value>A profile is a set of statistics that describes how often and for how long various parts of the program executed. Most profilers run inside your Python process.  If you’re inside a Python program you generally have pretty easy access to its stack, hence we can gather information about time spent in each level.</value><value>In this talk we’ll build a Python profiler from scratch and so learn about the dynamic nature of Python and how do well-known profilers such as cProfile. </value><value>We’ll also learn the difference between a tracing and a sampling profiler and which one to use in what circumstance.</value></description><date><value>Wednesday 25 July</value></date><tags><value>Performance</value><value>CPython</value></tags></item><item><title><value>Leadership of Technical Teams</value></title><author><value>Owen Campbell</value></author><description><value>Over the years, I’ve led, and been a member of, numerous technical teams on a wide variety of projects. Based on that experience, this talk will describe my personal observations on the role of the leader in that sort of team.</value><value>The talk will be in 5 sections:</value><value>There is no prior knowledge or experience required whatsoever. The talk is aimed equally at anyone  considering a leadership role for the first time or who has been doing so for many years.</value></description><date><value>Wednesday 25 July</value></date><tags><value>Best Practice</value><value>Business</value></tags></item><item><title><value>Let’s embrace WebAssembly!</value></title><author><value>Almar Klein</value></author><description><value>WebAssembly (WASM) is an open, low level binary format designed to be compact and run at native speed, while being memory-safe. WASM is primarily intended to run code in browsers, but its by no means limited to this. This makes it an interesting intermediate language (IR); code that compiles to WASM will (in the future) run basically anywhere. In short: WASM is coming and its great!</value><value>Unsurprisingly, WASM is being embraced by many communities, such as C++, Rust, Lua, and .NET. Sadly, there does not seem to be a lot of enthusiasm from the Python community yet…</value><value>Perhaps this is because Python is interpreted and can therefore not (easily) use WASM as a compilation target. It should be possible to compile a Python interpreter (like CPython or Pypy) to WASM and thereby run Python code in a browser. But the result would be pretty heavy-weight, so it would arguably not be a very practical.</value><value>Within the PPCI project (a pure Python compiler infrastructure) tooling has been developed to load, inspect, compile and even run WASM modules. This allows combining WASM and Python in new ways. In this talk we discuss two approaches by which the Python community might embrace WASM. </value><value>Firstly, various projects already exist that compile Python functions to other languages (e.g. Numba, PScript), and we have type annotations. If Python functions would be compiled to WASM, the resulting code would run in any WASM runtime (e.g. the browser). This will be demonstrated with an example, for which the resulting code can be run either in a browser or inside Python itself. In either case, it’s pretty darn fast.</value><value>Secondly, rather than </value><value>using</value><value> WASM, it might be interesting for Python to function as a platform to </value><value>run</value><value> and </value><value>bind</value><value> WASM modules. Two of Python’s greater strengths are its rich ecosystem and its ability to glue things together. Let’s build on that! Imagine creating an application that consists of multiple WASM modules, perhaps compiled from different languages, and binding these together into a single Python app.</value><value>This will be demonstrated with a game, which is written in Rust, compiled to WASM, and running inside a Python process. The WASM module consumes a drawing API, which is in this case provided by Python, and Python feeds the WASM module with user input.</value><value>I hope that this talk inspires other Pythonistas to think about the advantages that WASM can bring to our ecosystem, and also about the role that Python can play in the growing WASM ecosystem.</value></description><date><value>Wednesday 25 July</value></date><tags><value>Web</value><value>All Other Programming Languages</value><value>Community</value></tags></item><item><title><value>Iteration Inside Out - Python's Iteration Protocol</value></title><author><value>Naomi Ceder</value></author><description><value>Using for loops and list comprehensions in Python is basic and quite common, right? But how does iteration in Python actually work “under the hood”? The words “iterator” and “iterable” each occur over 500 times in the Python documentation, but what does an iterator actually </value><value>do</value><value>, as opposed to an iterable? And how do they do it? Learn the details as we turn the iteration protocol inside out, with live coded demonstrations along the way.</value><value>This talk will start from the way Python iterates of over a sequence, in comparison with iterating by index, like C. The key point of iterating over a sequence is that something needs to track which item in the sequence is next, which is something that Python’s  iteration protocol manages.</value><value>The iterable section will demonstrate creating a simple object that returns items by index (e.g., a fibonacci series), showing that </value><value>getitem</value><value> is really all you need for an iterable, since an iterator is created for such objects when iterated upon. BUT, this doesn’t answer the question of how Python keeps track of which item is next.</value><value>The iterator section answers that question by converting the iterable just created to an iterator - adding </value><value>iter</value><value> and </value><value>next</value><value> methods and showing how the iterator saves state and essentially drives the iteration protocol. </value><value>Having an accurate understanding of iteration protocol will help developing Pythonistas reason better about both iterating over existing objects and creating their own iterables and iterators.</value></description><date><value>Thursday 26 July</value></date><tags><value>Python general</value><value>Programming</value><value>Python 2</value><value>Python 3</value></tags></item><item><title><value>JavaScript for Python Developers</value></title><author><value>Žan Anderle</value></author><description><value>Having a hard time keeping track of where the modern JavaScript is going? Are you familiar only with jQuery and you want to know more? Or maybe you’re not familiar with JavaScript at all and want to learn it but you don’t know where to start? Then this talk is for you!</value><value>You’ll learn about modern JavaScript from a perspective of a Python developer. By the end of the talk you’ll know everything you need to know about the language, its ecosystem, and different tools and frameworks. You’ll be able to start using JavaScript more confidently and be familiar with different tools that are at your disposal.</value><value>We’ll go over all the things I wish I’d known when I first got started with JavaScript.:</value></description><date><value>Thursday 26 July</value></date><tags><value>Web</value><value>JavaScript</value><value>Web General</value><value>JavaScript Web Frameworks (AngularJS/ReactJS/...)</value></tags></item><item><title><value>Is your code tainted? Finding security vulnerabilities using taint-tracking.</value></title><author><value>Mark Shannon</value></author><description><value>“Taint tracking” is a technique used in code analysis to find security vulnerabilities and other problems.</value><value>Any data that comes from an untrusted source, for example a HTTP request, is treated as “tainted”.
If that “tainted” data is able to reach a vulnerable part of your code, then you have a problem.
Sophisticated code analysis tools can track this data, and reveal potential security problems.
Examples of the sort of problem that can be found include  cross-site scripting (XSS), code injection, SQL injection and others.</value><value>In this talk I will show how taint tracking analysis works in practice, introducing the concepts of source, sink and sanitizer.
I will then demonstrate using taint tracking to find a XSS vulnerability in a django app.
(We will chose a project that is designed to teach django security, where the vulnerability is deliberate.)</value><value>I will also explain how thinking in terms of “taint” can help you write safer code,
even without access to code analysis.</value><value>During this talk I will use the code analysis tools on lgtm.com to demonstrate the analysis.
lgtm.com is free to use for open-source projects. A paid version is available.</value></description><date><value>Thursday 26 July</value></date><tags><value>Best Practice</value><value>Security</value><value>Code Analysis</value></tags></item><item><title><value>Introduction to sentiment analysis with spaCy</value></title><author><value>Thomas Aglassinger</value></author><description><value>Sentiment analysis aims at extracting opinions from texts written in natural language, typically reviews or comments on social sites and forums. SpaCy already provides mechanisms for dealing with natural languages in general but does not offer means for sentiment analysis.</value><value>This talk gives a short introduction to sentiment analysis in general and shows how to extract topics and ratings by utilizing spaCy’s basic tools and extending them with a lexicon based approach and simple Python code to consolidate sentiments spread over multiple words.</value><value>Topic covered are:</value><value>Code examples are introduced and explained using a Jupyter notebook that can be used as basis for your own analysis.</value><value>As additional twist the analyzed texts are not in English but German to show that this approach can be used for multiple languages. No knowledge of German is required though because translations of the short examples sentences are provided.</value></description><date><value>Thursday 26 July</value></date><tags><value>Beginners</value><value>Notebook</value><value>Programming</value><value>Natural Language Processing</value></tags></item><item><title><value>Interoperability Rules for an European API Ecosystem: do we still need SOAP?</value></title><author><value>Roberto Polli</value></author><description><value>Italy is introducing a new API Ecosystem because the complexity of the old SOAP-based protocol was now a barrier for the creation of newer services.</value><value>This talk presents the Digital Transformation Team  ongoing work on an interoperability framework based on (REST) API, including:</value></description><date><value>Thursday 26 July</value></date><tags><value>Best Practice</value><value>Scaling</value><value>Privacy</value><value>RESTful</value><value>APIs</value></tags></item><item><title><value>How to write Rust instead of C, and get away with it (yes, it's a Python talk)</value></title><author><value>Antonio Verardi</value></author><description><value>Have you ever tried optimizing a super-slow Python application and thought: “Oh! I wish I could just write this bit it in Rust”? Well, turns out you can!
We will show you how Rust is a better alternative than C to make your programs lightning fast, and how to get away with it; without your users even noticing.</value><value>As Infrastructure Engineers at Yelp, the challenge we face everyday is: scale. Yelp is mostly a Python shop and while this is great for development velocity, our work often revolves around making Python applications run faster. Until now, we have been using different techniques: faster interpreters, or, more often, C code.</value><value>Given its safety guarantees, performance and promise of better tooling than C, we decided we had to give Rust a try. The initial results helped reinforce that there was a lot of opportunity for Rust to play an important role in our production code.</value><value>Yelp heavily relies on the Apache Avro (https://avro.apache.org/) serialization format for its internal infrastructure. During the talk, we will show how we implemented an Avro serialization/deserialization library in Rust, how we were able to call it from Python (and in theory from any other language) with very little code, using tools such as cbindgen (https://github.com/eqrion/cbindgen/), CFFI (https://cffi.readthedocs.io/en/latest/) and Milksnake (https://github.com/getsentry/milksnake/).</value><value>This talk would outline how easy it is to write performant code in a language like Rust and call it from Python applications without users even realizing it, making this a great solution for production services.</value></description><date><value>Wednesday 25 July</value></date><tags><value>Performance</value><value>CPython</value><value>C-Languages</value></tags></item><item><title><value>How to Write Deployment-friendly Applications</value></title><author><value>Hynek Schlawack</value></author><description><value>The DevOps movement gave us many ways to put Python applications into production. But should your application care? Should it need to know whether it’s running on your notebook, on a server, in a Docker container, or in some cloud platform as a service?</value><value>It should not, because environment-agnostic applications are easier to test, easier to deploy, easier to handle, and easier to scale.</value><value>But how can you practically structure and configure your applications to make them indifferent to the environment they run in? How do secrets fit into the picture? And where do you put that log file?</value><value>By the end of this talk you’ll know the tools and techniques that enable you to write such Python applications and you’ll be ready for the next big change. </value></description><date><value>Thursday 26 July</value></date><tags><value>Best Practice</value><value>Scaling</value><value>Deployment/Continuous Integration and Delivery</value><value>Architecture</value><value>DevOps general</value></tags></item><item><title><value>Integration Tests with Super Powers</value></title><author><value>Alexandre Figura</value></author><description><value>You are maybe like me: I never learned at school how to write tests. My teachers gave me at first a broad overview of computer history. Then, they explained me some basic design patterns. And to finish, I often had to write more or less basic programs, to validate and demonstrate my skills. Not the kind of code I would be really proud of today: the procrastinator monkey living in my head at this time was more thinking about planning my summer holidays, rather than writing Ninja code!</value><value>And to make things worse, my studies focused on network and system engineering. Not software architecture. Funny story, because I decided to become programmer a couple of years later…</value><value>What I realize now is that I don’t have as much time as before to learn. And in a world driven by business, where time is money, and where tradeoffs are the rule, there is rarely enough money to write both shiny new features and a complete test suite.</value><value>People who practice Test-Driven Development know how complicated it can be to write proper tests. TDD is often discouraging at first: the learning curve is steep. But this problem also exists in the testing world in general. Because writing good tests is hard, many beginners get headaches trying to reach this goal. How to convince project managers to have more time for writing tests in these conditions…</value><value>But “le jeu en vaut la chandelle” as we say in French ("the juice is worth the squeeze"). Well tested applications are not only easier to maintain and extend. They also have in general a better API. That’s what we will see in this talk, by focusing on how to write integration tests. Our journey will begin with a presentation of different testing strategies. We will then jump to the practical part, using Pytest, interface testing , dependency injections and stubs, amongst many others. And because we want to add nice buzzwords on our resume after EuroPython, we will finish this talk by automating the whole with Docker Compose.</value></description><date><value>Friday 27 July</value></date><tags><value>Docker</value><value>Testing</value><value>Tooling</value><value>Test Libraries (pyTest/node/...)</value></tags></item><item><title><value>Industrial Machine Learning Pipelines with Python &amp; Airflow</value></title><author><value>Alejandro Saucedo</value></author><description><value>This talk will provide key insights on the learnings I have obtained throughout my career building &amp; deploying machine learning systems in critical environments across several sectors. I will provide a deep dive on how to build scalable and distributed machine learning data pipelines using Airflow with a Celery backend. I will also compare Airflow with other technologies available out there and how it differentiates, such as Luigi, Chronos, Pinball, etc. If you attend the talk, you will obtain an understanding on the solid fundamentals of Airflow, together with its caveats and walk-arounds for more complex use-cases. As we proceed with the examples, I will cover the challenges that you will run into when scaling Machine Learning systems, and how Airflow can be used to address these using a manager-worker-queue architecture for distributed processing with Celery.  By the end of this talk you will have the knowledge required to build your own industry-ready machine learning pipelines to process data at scale, and I will provide further reading resources so people are able to implement the knowledge obtained almost right away.</value></description><date><value>Thursday 26 July</value></date><tags><value>Best Practice</value><value>Deep Learning</value><value>Distributed Systems</value><value>Big Data</value><value>Machine-Learning</value></tags></item><item><title><value>How to make money using Python - Unused potential in the Enterprise World</value></title><author><value>Marc-Andre Lemburg</value></author><description><value>Python has gained quite some traction in the web development world and more recently as the goto language for anything that has to do with data science. However, it’s use in the enterprise world of applications is rather limited.</value><value>Based on the author’s many years experience in working in enterprise environments, the talk will demonstrate areas in the business application space where Python has significant advantages over other languages, but which are currently dominated by applications written in Java, C++ or C#.</value><value>There are huge opportunities out there for companies to excel at and use the Python advantage to their benefit. If you are looking for a lead idea to kick start your Python business (and you have the resources to invest into marketing), this talk is for you.</value></description><date><value>Thursday 26 July</value></date><tags><value>Best Practice</value><value>Python general</value><value>Business</value><value>Use Case</value></tags></item><item><title><value>EduBlocks - Making the transition to Python easier!</value></title><author><value>Joshua Lowe</value></author><description><value>Looking for ways to make the transition from block based programming to Python easier, then look no further!  In this talk 14 year old Josh will introduce you to his project called EduBlocks, which is a drag and drop version of Python 3 that he has created to help teachers introduce programming languages, like Python, to children at an earlier age.
The goal of the project is to make the transition from block based programs like Scratch to Python easier for students and teachers, as presently there is no drop-in solution that bridges this gap. Josh will share his journey so far with you, from how he came up with the idea when he was only 11 years old, the developments along the way, the exciting plans for the future and how schools today in over 72 different countries  around the world are making use of EduBlocks on the Raspberry Pi and micro:bit. </value></description><date><value>Friday 27 July</value></date><tags><value>MicroPython</value><value>Education</value><value>Python 3</value><value>Community</value><value>Open-Source</value></tags></item><item><title><value>How to develop your project from an idea to architecture design in 50 minutes</value></title><author><value>Anastasiia Tymoshchuk</value></author><description><value>Have you ever asked yourself: </value><value>
- how and where to start developing a new project from the scratch, </value><value>
- how to choose main components of a new software or how to develop a new feature, </value><value>
- how to decide whether your project needs refactoring, </value><value>
- how to avoid repeating the same issues? </value><value>
This talk will answer all these questions.  </value><value>You will learn:
- how to develop the idea from scratch, </value><value>
- how to use principles of Object Oriented Analysis and Design, </value><value>
- how to show amazing architecture design.  </value><value>You will get better understanding where and how to start, analyze and decouple the system, create a clean, clear and extendable architecture.</value><value>My slides are here: https://atymo.me/projects/presentations/FromIdeaToDesign/</value></description><date><value>Wednesday 25 July</value></date><tags><value>Software Design</value><value>Data Structures</value><value>Architecture</value><value>Code Analysis</value><value>Use Case</value></tags></item><item><title><value>Domain-Driven Design Patterns in Python</value></title><author><value>Robert Smallshire</value></author><description><value>Domain-Driven Design (DDD) is an approach to software development that emphasises high-fidelity modelling of the problem domain, and which uses a software implementation of the domain model as a foundation for system design. This approach helps organize and minimize the essential complexity of your software.  DDD has been used with success within the traditional enterprise programming ecosystems of Java and .NET, but has seen only limited adoption in the Python community.</value><value>In this talk we introduce Python programmers to the core tactical patterns of DDD and show how they can be realised in idiomatic Python, freeing the most valuable parts of your system – the domain model – from onerous dependencies on particular databases or application frameworks.</value><value>In this talk we share what we’ve learned from applying DDD in Python to large projects.</value></description><date><value>Friday 27 July</value></date><tags><value>Software Design</value><value>Abstractions</value><value>Programming</value><value>Architecture</value><value>Databases</value></tags></item><item><title><value>Easy interactive data applications with Dash</value></title><author><value>Matteo Guzzo</value></author><description><value>Plotly Dash is a Python  framework for building interactive dashboards and web data applications, based on Flask, React.js, and Plotly. It allows a python-only approach to something that previously required knowledge of Javascript, heavily reducing the overhead required to create a web application. I’ll show how easy it is to set up a small interactive web app using data from the Twitch API and to expand it at will, using only Python. </value></description><date><value>Friday 27 July</value></date><tags><value>Web</value><value>Visualization</value><value>Data Science</value><value>Scientific Libraries (Numpy/Pandas/SciKit/...)</value><value>Web Servers and MicroFWs (Flask/Tornado/Nginx/...)</value></tags></item><item><title><value>Django structure for scale and longevity</value></title><author><value>Radoslav Georgiev</value></author><description><value>Django is great.</value><value>But as we add new features, as our dev team grows &amp; the software needs to be stable on production, things can get quite messy.</value><value>We are going to look at some common patterns, derived from experience, on how to structure your Django project for scale and longevity.</value><value>Main topics are:</value><value>We are going to talk about when to rely on existing abstraction so it’s actually helpful &amp; when to avoid existing abstraction, and code things ourselves.</value><value>The examples showed in this talk are derived from working with Django in the last 5 years on projects with:</value><value>Key takeaways from the talk:</value><value>The talk is great for all levels of Django knowledge - from beginners to advanced users &amp; teams.</value><value>The main way of getting the point across is going to be by showing regular code, talking how it can get messy &amp; then following up with examples of improving that code. Hopefully this talk will start a lot of discussion afterwards.</value><value>Breakdown of the talk:</value></description><date><value>Thursday 26 July</value></date><tags><value>Best Practice</value><value>Clean Code</value><value>Python 3</value><value>failures/mistakes</value><value>Django</value></tags></item><item><title><value>Developing in a black hole: vim, tmux, httpie and jq</value></title><author><value>Francisco Javier Aceituno Lapido</value></author><description><value>“Productivity depends on the knowledge you have of the tools used.”</value><value>Do you feel that the development tools you use are heavy? 
Do you think that they make you less productive?
Do you want to have maximum productivity without lifting your hands from the keyboard? </value><value>If so, this talk is for you.</value><value>In this talk, I will show what are the tools I use every day in software development at OnTruck and how you can integrate</value><value>the text editor, vim
the terminal multiplexer, tmux
the HTTP client, httpie
and the json processor, jq</value><value>to get a very productive development environment, all running on a simple terminal.</value></description><date><value>Thursday 26 July</value></date><tags><value>Development</value><value>Agile</value><value>Unix</value><value>Tooling</value><value>Command-Line</value></tags></item><item><title><value>Deep Learning with PyTorch for Fun and Profit</value></title><author><value>Alexander Hendorf</value></author><description><value>There are all these great blog posts about Deep Learning describing all that awesome stuff.  - Is it all that easy? Let’s check!</value><value>We’ll look into: style transfer (making a picture look like painting), speech generation (like Siri or Alexa) and text generation (writing a story).
In this talk I’ll describe the whole journey: A fun ride from the idea to the very end including all the struggles, failures and successes.</value><value>Steps, we’ll cover:</value><value>The data challenge: get the data ready</value><value>Have it run on your Mac with PyTorch and an eGPU</value><value>Creating a character-level language models with an Recurrent Neural Network</value><value>Creating a text generator</value><value>Creating artwork</value></description><date><value>Thursday 26 July</value></date><tags><value>Best Practice</value><value>Case Study</value><value>Natural Language Processing</value><value>failures/mistakes</value><value>Deep Learning</value></tags></item><item><title><value>Django queries optimization</value></title><author><value>Ivaylo Donchev</value></author><description><value>Collecting data from multiple Models is a common flow in Django development. In most of the cases </value><value>prefetch_related</value><value> and </value><value>select_related</value><value> do all of the job for optimizing the queries.</value><value>When the models structure become complicated enough and we’ve put our logic for collecting these items in models’ properties we suddenly cannot use </value><value>prefetch_related</value><value> or </value><value>select_related</value><value> anymore. Our View ‘s queries count depends on selected items count with high complexity. The problem is in the selection logic: we’ve implemented an algorithm which calculates something and we cannot prefetch or select all of the objects. </value><value>So this talk’s purpose is: To show how to use Django ORM to move the selection logic, the calculations, the aggregations over group of objects from our python code to our database and moving all of the logic for this in one place - the queryset and test it easily.</value></description><date><value>Thursday 26 July</value></date><tags><value>Best Practice</value><value>Django</value><value>Databases</value><value>PostgreSQL</value><value>APIs</value></tags></item><item><title><value>Data is not flat</value></title><author><value>Alisa Dammer</value></author><description><value>Feature engineering and model training often comes hand in hand. Some tasks have an overwhelming amount of high dimensional data, some tasks have little data or very low-dimension data. 
This talk targets the latter problem: what can be done with the data itself to significantly improve the model performance and when manual feature engineering does make sense. </value><value>
A sample case of Classification problem with NN will be presented
The goal of the talk is to remind about something every person working with the data thinks and probably uses.
 Slides, Jupyter notebook with the example, test and train sets, NN configuration file are available on:
https://github.com/Alisa-lisa/conferences</value></description><date><value>Thursday 26 July</value></date><tags><value>Algorithms</value><value>Data Science</value><value>Data</value><value>Machine-Learning</value></tags></item><item><title><value>Debugging Your Code with Data Visualization</value></title><author><value>James Saryerwinnie</value></author><description><value>Let’s face it.  Sometimes our code just isn’t working how we expect it to work.
When this happens, we fall back to our trusty tools to help us
debug: pdb, the logging module, or even simple print statements.  But sometimes
that just isn’t enough, we still can’t figure out why our code is broken.
We need something more.</value><value>There are a number of fantastic libraries in python for
creating data visualizations.  These libraries are commonly used for
analyzing and visualizing large sets of data, but don’t see as much
usage when it comes to helping us write and debug our code.</value><value>What if we could combine the two?  What if we could leverage data
visualization tools to understand what our program is doing?  Sometimes, by
seeing a visual representation of what our code is doing, it becomes much
easier to understand why our code is not working.  We don’t need to create
publication quality graphs and charts, we just need to generate quick, one-off
visualizations to understand what our code is doing.  What we want is the “data
visualization” equivalent to print() statements.</value><value>To help illustrate this point, I will walk through some of the hardest
bugs I’ve had to track down while working on projects such as the AWS CLI and
boto3, the AWS SDK for Python.  For each bug, I’ll show you how I was able to
leverage data visualization techniques to troubleshoot and fix these bugs.
Come learn how to debug more efficiently by leveraging data visualization.</value></description><date><value>Thursday 26 July</value></date><tags><value>Public Cloud (AWS/Google/...)</value><value>Visualization</value><value>Debugging</value><value>Case Study</value></tags></item><item><title><value>Cython to speed up your Python code</value></title><author><value>Stefan Behnel</value></author><description><value>Cython</value><value> is not only a very fast and comfortable way to talk to native code and libraries, it is also a widely used tool for speeding up Python code. The Cython compiler translates Python code to C or C++ code, and applies many static optimisations that make Python code run visibly faster than in the interpreter. But even better, it supports static type annotations that allow direct use of C/C++ data types and functions, which the compiler uses to convert and optimise the code into fast, native C. The tight integration of all three languages, Python, C and C++, makes it possible to freely mix Python features like generators and comprehensions with C/C++ features like native data types, pointer arithmetic or manually tuned memory management in the same code.</value><value>This talk by a core developer introduces the Cython compiler by interactive code examples, and shows how you can use it to speed up your Python code. You will learn how you can profile a Python module and use Cython to compile and optimise it into a fast binary extension module. All of that, without losing the ability to run it through common development tools like static analysers or coverage test tools.</value></description><date><value>Thursday 26 July</value></date><tags><value>Cython</value><value>Compiler and Interpreters</value><value>C-Languages</value><value>Performance</value></tags></item><item><title><value>Creating Solid APIs</value></title><author><value>Rivo Laks</value></author><description><value>Increasingly, our apps are used not by humans but by other apps - via their APIs. Thus it is increasingly important that your APIs are well-designed and easy to consume for other developers.</value><value>Adding a few API endpoints to your application for internal consumption is easy. Creating APIs that other developers will love to use is a much harder problem.
You’ll need to think about solving variety of topics such as versioning, authentication, response structure, documentation and more. There are existing good practices for each of them, but often developers who haven’t done a lot of API work aren’t familiar with them.</value><value>My talk will show how to find reasonable solutions for those problems.
I will talk about importance and intricacies of good documentation and why auto-generating it from your code is useful. I’ll show how to make use of familiarity by using standards such as JSON API and show benefits brought by its standardized response structure that makes lives of 3rd-party developers easier.</value><value>Authentication will be discussed, including introduction to OAuth2. I’ll talk about when OAuth2 is a good choice and when not, as well as dig into some trickier parts of it.
We’ll then move on to versioning and how you can change your API without breaking all existing apps.
Finally we’ll wrap it all up by looking at some major APIs that are using the same principles.</value></description><date><value>Friday 27 July</value></date><tags><value>Best Practice</value><value>RESTful</value><value>Web General</value><value>APIs</value></tags></item><item><title><value>Creating a Culture of Software Craftsmanship </value></title><author><value>Keith Harrison</value></author><description><value>This time it’ll be different. It’s a green field project and you’ve learned from your mistakes. You’re not going to make the same ones again.</value><value>Fast forward several months and you’re in a mess. A change in one place causes bugs in a completely different place. Adding a simple feature means making changes all over your application. Progress is getting slower and slower. You’re lost in complexity.</value><value>Why does this keep happening and what can we do about it? </value><value>This talk will try to answer those questions.</value><value>I’ll show how complex software really is, define the different types of complexity found in software, explain the issues complexity causes and discuss different techniques we can use to manage it.</value><value>This talk is suitable for anyone who has worked or will work on a software development project. No development experience is required.</value></description><date><value>Friday 27 July</value></date><tags><value>Best Practice</value><value>Agile</value><value>Clean Code</value></tags></item><item><title><value>Code Review Skills for Pythonistas</value></title><author><value>Nina Zakharenko</value></author><description><value>As teams and projects grow, code review becomes increasingly important to support the maintainability of complex codebases. In this talk, I’ll cover guidelines for writing consistent python code beyond pep8, how to look out for common python gotchas, and what python tools are available to automate various parts of the review process. Most importantly, I’ll cover the human aspect of code reviews - how we can be better at approaching reviews with empathy and understanding from the perspective of both a reviewer and a submitter. Following these successful code review practices will lead to happier teams and healthier code bases.</value><value>This talk is useful for python developers with any amount of experience. No prerequisite knowledge is necessary. 
- For those who are just starting out, it will be a great general overview.
- Intermediate developers may not know about the wide variety of tooling that’s available.
- Advanced developers will learn techniques for performing code reviews with empathy.</value><value>This talk will enable you to have better code reviews on your teams at work, and a better approach to code reviews in open source projects. You’ll leave with 3 main takeaways:
 1. Code Reviews are most effective when conducted with empathy. If you do reviews with growth and learning in mind, they become tools for sharing knowledge instead of an opportunity to bruise egos or show off esoteric knowledge.
 2. Python has powerful tooling available for code reviews such as pep8 as a style guide, pylint as a linter, coverage.py to identify test coverage, and vulture to identify dead code.
 3. That python style guides beyond pep8 have clear benefits in terms of producing more consistent code that’s easier to review and easier to maintain.</value></description><date><value>Thursday 26 July</value></date><tags><value>Development</value><value>Clean Code</value><value>Programming</value><value>Open-Source</value><value>Code Analysis</value></tags></item><item><title><value>Change music in two epochs</value></title><author><value>Marcel Raas</value></author><description><value>This talk is about applying deep learning to music. We will look at the raw music data and discover the following:</value><value>Instead of applying it to existing music we will generate our own music using some simple musical rules. The benefit of this is that we are in control of the complexity and we know exactly what is being played. We start out simple and then start adding more instruments, different timbres, etc. As we go up in complexity, we shall see how to adapt our models to be able to deal with it. This gives interesting insights in what structures in deep nets work well.</value><value>I will show:</value><value>For more info, see the github repository at https://github.com/marcelraas/music-generator</value></description><date><value>Friday 27 July</value></date><tags><value>Deep Learning</value><value>Algorithms</value><value>Machine-Learning</value><value>Scientific Libraries (Numpy/Pandas/SciKit/...)</value></tags></item><item><title><value>Building new NLP solutions with spaCy and Prodigy</value></title><author><value>Matthew Honnibal</value></author><description><value>Commercial machine learning projects are currently like start-ups: many projects fail, but some are extremely successful, justifying the total investment. While some people will tell you to “embrace failure”, I say failure sucks — so what can we do to fight it? In this talk, I will discuss how to address some of the most likely causes of failure for new Natural Language Processing (NLP) projects. My main recommendation is to take an iterative approach: don’t assume you know what your pipeline should look like, let alone your annotation schemes or model architectures. I will also discuss a few tips for figuring out what’s likely to work, along with a few common mistakes. To keep the advice well-grounded, I will refer specifically to our open-source library spaCy, and our commercial annotation tool Prodigy.</value></description><date><value>Thursday 26 July</value></date><tags><value>Deep Learning</value><value>Data Science</value><value>Natural Language Processing</value></tags></item><item><title><value>CatBoost - the new generation of Gradient Boosting</value></title><author><value>Anna Veronika Dorogush</value></author><description><value>Gradient boosting is a powerful machine-learning technique that achieves state-of-the-art results
in a variety of practical tasks. For a number of years, it has remained the primary method for
learning problems with heterogeneous features, noisy data, and complex dependencies: web search,
recommendation systems, weather forecasting, and many others.
CatBoost (http://catboost.yandex) is a new open-source gradient boosting library, that outperforms existing publicly available implementations of gradient boosting in terms of quality. It has a set of addional advantages.</value><value>CatBoost is able to incorporate categorical features in your data (like music genre, URL, search query, etc.) in predictive models with no additional preprocessing. For more details on our approach please refer to our NIPS 2017 ML Systems Workshop paper (http://learningsys.org/nips17/assets/papers/paper_11.pdf).</value><value>CatBoost inference is 20-60 times faster then in other open-source gradient boosting libraries, which makes it possible to use CatBoost for latency-critical tasks.</value><value>CatBoost has the fastest GPU and multi GPU training implementations of all the openly available gradient boosting libraries.</value><value>CatBoost requires no hyperparameter tunning in order to get a model with good quality.</value><value>CatBoost is highly scalable and can be efficiently trained using hundreds of machines</value><value>The talk will cover a broad description of gradient boosting and its areas of usage and the differences between CatBoost and other gradient boosting libraries. We will also briefly explain the details of the proprietary algorithm that leads to a boost in quality.</value></description><date><value>Thursday 26 July</value></date><tags><value>Big Data</value><value>Data Science</value><value>Open-Source</value><value>Machine-Learning</value></tags></item><item><title><value>Bytecodes and stacks: A look at CPython’s compiler and its execution model</value></title><author><value>Petr Viktorin</value></author><description><value>So, you wrote some Python code. What needs to happen before it starts running? And once it’s running, how does Python keep track of what it’s doing?</value><value>I’ll talk about CPython’s tokenization, parsing, bytecode and its serialization and cache, the stack-based virtual machine, line number tables, and code, frame and function objects.</value><value>Don’t worry if you’ve never heard of these concepts. While even experts should learn something new, the talk is aimed at anyone who’s worked on a Python project or two.</value></description><date><value>Friday 27 July</value></date><tags><value>Compiler and Interpreters</value><value>CPython</value></tags></item><item><title><value>Bridging the Gap: from Data Science to Production</value></title><author><value>Florian Wilhelm</value></author><description><value>A recent but quite common observation in industry is that although there is an overall high adoption of data science, many companies struggle to get it into production. Huge teams of well-payed data scientists often present one fancy model after the other to their managers but their proof of concepts never manifest into something business relevant. The frustration grows on both sides, managers and data scientists.</value><value>In my talk I elaborate on the many reasons why data science to production is such a hard nut to crack. I start with a taxonomy of data use cases in order to easier assess technical requirements. Based thereon, my focus lies on overcoming the two-language-problem which is Python/R loved by data scientists vs. the enterprise-established Java/Scala. From my project experiences I present three different solutions, namely 1) migrating to a single language, 2) reimplementation and 3) usage of a framework. The advantages and disadvantages of each approach is presented and general advices based on the introduced taxonomy is given. </value><value>Additionally, my talk also addresses organisational as well as problems in quality assurance and deployment. Best practices and further references are presented on a high-level in order to cover all facets of data science to production.</value><value>With my talk I hope to convey the message that breakdowns on the road from data science to production are rather the rule than the exception, so you are not alone. At the end of my talk, you will have a better understanding of why your team and you are struggling and what to do about it.    </value></description><date><value>Wednesday 25 July</value></date><tags><value>Best Practice</value><value>Data Science</value><value>Clean Code</value><value>DevOps general</value><value>Business</value></tags></item><item><title><value>Building a Naive Bayes Text Classifier with scikit-learn</value></title><author><value>Obiamaka Agbaneje</value></author><description><value>Machine learning algorithms used in the classification of text are Support Vector Machines, k Nearest Neighbors but the most popular algorithm to implement is Naive Bayes because of its simplicity based on Bayes Theorem.</value><value>The Naive Bayes classifier is able to memorise the relationships between the training attributes and the outcome and predicts by multiplying the conditional probabilities of the attributes with the assumption that they are independent of the outcome.  It is popularly used in classifying data sets that have a large number of features that are sparse or nearly independent such as text documents.</value><value>In this talk, I will describe how to build a model using the Naive Bayes algorithm with the scikit-learn library using the spam/ham youtube comment dataset from the UCI repository.  Preprocessing techniques such as Text normalisation and Feature extraction will be also be discussed.</value></description><date><value>Thursday 26 July</value></date><tags><value>Scientific Libraries (Numpy/Pandas/SciKit/...)</value><value>Data</value><value>Natural Language Processing</value></tags></item><item><title><value>Building a Question Answering System using Deep Learning Techniques </value></title><author><value>Rricha Jalota</value></author><description><value>Question Answering is an active area of research where the goal is to provide brief and crisp answers to natural language questions. Given a question and a text passage, the task is to answer the question based on the information given in the passage. Traditionally, NLP techniques like noise removal, tokenization,  dependency parsing, named entity recognition etc. are used to extract an answer from the passage. With Deep Learning techniques gaining traction, the focus has now shifted to how neural network architectures can improve the accuracy of Question Answering Systems. 
This talk will help the audience understand how QA systems work and enable them to build one on their own! 
Talk Outline :
1. Introduction to Question Answering(Reading Comprehension, in particular)
2. Discuss available datasets
3. Baseline Approach to solving the problem using NLP techniques
4. Walk through the pipeline of deploying Deep Learning models (i.e. preprocessing dataset, generating  word embeddings, building an encoder-decoder model using LSTMs, attention networks and evaluating it) </value><value>Pre-requisites: Python, Basics of Neural Networks and NLP, Keras   </value></description><date><value>Friday 27 July</value></date><tags><value>Scientific Libraries (Numpy/Pandas/SciKit/...)</value><value>Data Science</value><value>Natural Language Processing</value><value>Jupyter/iPython</value><value>Deep Learning</value></tags></item><item><title><value>Autism in development</value></title><author><value>Ed Singleton</value></author><description><value>Autism is a condition that correlates with engineering.  Many people in the industry are diagnosed autistic, undiagnosed autistic, or near autistic.  There are many lessons that can be learned from how to deal with autistic people that are very useful in our industry.  These lessons are often also useful when dealing with non-autistic people.  There are many lessons on how to deal with your own autism that are useful to undiagnosed autistic people and near autistic people.</value><value>We will cover the basics of autism, and related conditions.  We will cover simple tricks that can make discussions and changes easier in workplaces ("10 Simple tricks to make your autistic colleagues shout less!").  We will also cover techniques you can use for dealing with your own autistic tendencies ("100 coping mechanisms to pretend that you are normal!").</value><value>I am a late diagnosis autistic person myself, and will discuss the difference between knowing and not knowing that you are autistic.  </value></description><date><value>Wednesday 25 July</value></date><tags><value>The Answer to Life the Universe and Everything Else</value><value>Diversity</value><value>workforce</value><value>Community</value></tags></item><item><title><value>Bad hotel again? Find your perfect match!</value></title><author><value>Elisabetta Bergamini</value></author><description><value>For most travellers, online reviews play a major role when it comes to choosing which hotel to stay in. But can we actually trust a hotel review? And if yes, how can we select which are the most meaningful and interesting for us among the billions available in platforms such as Booking.com, Tripadvisor, Facebook (just to mention a few)?
For 10 years now, at TrustYou we have built processes that analyze terabytes of hotel reviews at a global scale, and strive to understand what people complain about or like in the hotels worldwide.
Dealing with a huge amount of reviews written in tens of different languages - each having its own subtle shades of meanings - is the challenge we work on everyday. In this talk, we will show what goes on behind the scenes of the TrustYou Metareview and dive into the technologies and the algorithms that allow us to provide travellers with all the information they need to find the perfect hotel. </value></description><date><value>Wednesday 25 July</value></date><tags><value>Data Science</value><value>Machine-Learning</value><value>Natural Language Processing</value></tags></item><item><title><value>Asyncio in Python 3.7 and 3.8.</value></title><author><value>Yury Selivanov</value></author><description><value>The talk is aimed to give attendees a clear picture of new asyncio features in Python 3.7 and give an idea of what to expect in Python 3.8.  As a CPython core developer and the lead asyncio maintainer I believe I have a unique perspective to share with EuroPython guests.</value><value>Python 3.7 boasts a number of new amazing features: </value><value>The first part of the talk will be focusing on new APIs to explain them and give ideas how they can be used in production.</value><value>The second part of the talk will be focused on what to expect to see in asyncio in Python 3.8:</value><value>Some of these ideas are borrowed from Trio and Curio (two other popular async/await Python frameworks), many are borrowed from languages like Erlang and Scala. One thing I can say for certain is that 3.8 will be the biggest and most interesting asyncio release ever!</value></description><date><value>Wednesday 25 July</value></date><tags><value>Best Practice</value><value>Distributed Systems</value><value>ASYNC / Concurreny</value><value>failures/mistakes</value><value>Use Case</value></tags></item><item><title><value>Automating testing and deployment with Github and Travis</value></title><author><value>Alex Grönholm</value></author><description><value>Maintaining an open source project can be a drag. Especially when you have dozens of them. Besides being tedious , making a new release can be a very error prone process. Maybe you forgot to run the full test suite, forgot to update the release version or tag the release on VCS or maybe you bungled up the upload to PyPI. Fortunately there are things you can do to eliminate entire categories of human errors in the release process – by automating them.</value><value>This talk uses a trivial project as an example to present a release pipeline, from packaging and testing to deployment, that only requires you to push a git tag to your Github repository. Automation then takes care of the rest and you end up with a new release on PyPI (yay!).</value><value>This talk is mostly useful for open source developers using Github to house their projects, as it heavily leans on Travis, its continuous integration provider . You also need to be at least somewhat familiar with Python packaging and testing in order to follow along. Familiarity with common tools like pytest and tox helps but is not strictly required.</value></description><date><value>Friday 27 July</value></date><tags><value>Best Practice</value><value>Packaging</value><value>Deployment/Continuous Integration and Delivery</value><value>Tooling</value></tags></item><item><title><value>asyncio in Practice: We Did It Wrong</value></title><author><value>Lynn Root</value></author><description><value>This talk is aimed at those that have at least intermediate experience in Python and have played around with asynchronous Python using asyncio or other libraries. I want the audience to learn from my mistakes! For instance, how easy it is to get into “callback hell” (and how to avoid/get out of it), how to screw up thread safety and deadlock yourself, and making code async but not actually concurrent.</value><value>I’ll talk through some anti-patterns and best practices that I learned the hard way. This includes proper concurrency, calling coroutines from synchronous code, working with threads and threadsafety, properly shutting down an async programs, and hidden “gotchas”.</value></description><date><value>Thursday 26 July</value></date><tags><value>Best Practice</value><value>ASYNC / Concurreny</value><value>failures/mistakes</value><value>Multi-Threading</value></tags></item><item><title><value>All You Need is Pandas: Unexpected Success Stories</value></title><author><value>Dimiter Naydenov</value></author><description><value>Learning to use the awesome Pandas toolkit helped me immensely in lots of ways. Finding novel,
efficient solutions to complex day-to-day problems with Pandas not only saves time, but can be fun
and rewarding experience.</value><value>In this talk I’ll present use cases I had to solve, but the “traditional” approach proved tough
and/or otherwise frustrating implement nicely. Since I was just starting to learn Pandas, decided to
try an alternative solution with it. What I learned changed the way I think about data processing
with Python, and it only got better since!</value><value>The use cases deals with extracting pen strokes from handwritten SVG samples, and recomposing them
into reusable letters and numbers. When you need to compare each stroke to all others, often more
than once, resulted in inefficient, slow, and hard to maintain code. Even a naive Pandas approach
with loops helped to reduce both the memory footprint, and improve the performance considerably!
Improving the implementation further, vectorizing inner loops, and taking advantage of multi-index
operations, I managed to get the same results, using less memory and a lot faster (by orders of
magnitude).</value></description><date><value>Wednesday 25 July</value></date><tags></tags></item><item><title><value>Asyncio in production</value></title><author><value>Hrafn Eiriksson</value></author><description><value>Much has been written about asynchronous programming in Python, especially after the introduction of asyncio into the standard library of the language. We’ve all seen the benchmarks that tell us how asyncio-powered web servers massively outperform their non-asyncio counterparts for trivial routes and we’ve seen the articles that tell us how to make the move from flask to aiohttp and why we should. </value><value>Despite all of this, the question remains: How is it to use asyncio in a production setting? What are the pain points of migrating a large application to use asyncio? How does the performance of this real-world application differ after the migration? Is it still just unicorns and fairy tales? The goal of this talk is to address these questions. </value><value>If you are curious about taking the step from asyncio-dabbling to creating actual mission critical software systems in asyncio; this is the talk for you!</value></description><date><value>Wednesday 25 July</value></date><tags><value>Python 3</value><value>ASYNC / Concurreny</value></tags></item><item><title><value>Air Quality &amp; Python: Developing Online Analysis Tools</value></title><author><value>Douglas Finch</value></author><description><value>Poor surface air quality has a range of implications for human health and the economy. Without concerted mitigation efforts, trends in urbanisation and aspirations for progressive economic growth will result in poorer levels of air quality. Analysing and interpreting the incoming data streams from heterogeneous air quality measurement stations is critical for tackling the problem and for developing early warning systems. I am using Python to develop a set of online analysis tools (ukatmos.org) to enable the public to quickly and easily plot air quality data in many ways, effectively freeing up information that is already publicly available but in awkward formats and often involves development of code. We anticipate these tools will also support data science classes at school, and can speed up scientific research by minimizing effort in repeating analyses. </value><value>This talk will cover how the tools integrate numerous Python libraries (e.g. Pandas and NumPy), the Django web framework, the Plot.ly tools for creating interactive graphs, and SQL to address the large data volumes. Developing these Python tools in an adaptive and scalable way allows it to grow as more data become available, e.g. satellite observations. Adaptability also includes evolving user requirements. This project will also be developed into a Python library allowing the user to easily use the online analysis tools from an offline Python environment. </value></description><date><value>Friday 27 July</value></date><tags><value>Case Study</value><value>Data Science</value><value>Django</value><value>Web</value><value>Science</value></tags></item><item><title><value>Addressing multithreading and multiprocessing in transparent and Pythonic ways</value></title><author><value>David Liu</value></author><description><value>With the increase in computing power, harnessing and controlling one’s code out of the single-threaded realm becomes an ever-increasing problem, coupled with the desire to stay in the Python layer. With the recent tools and frameworks that have been published, escaping the GIL cleanly is much easier than before, allow one’s Python code to effectively utilize multi-core and many core architectures in the most Pythonic ways possible. In this talk, learn about how to utilize static multiprocessing for process pinning, and effectively balancing thread pools with a monkey-patched import of threading modules.</value><value>Overview:</value><value>Introduction to multithreading and multiprocessing in Python</value><value>History of multithreading+multiprocessing in Python, classic frameworks </value><value>Problems that can occur (oversubscription, nested parallelism issues, process hopping, pool resource on shared machines) </value><value>Python accessing bigger hardware over the last few years (28+ cores, etc) </value><value>When to stay in the GIL, and when to escape it </value><value>The advantages and safety of the GIL</value><value>Python-level exiting of the GIL; analysis of when to return to single-threaded, and when threading is a deceivingly bad idea</value><value>Accountability of frameworks that natively exit the GIL</value><value>The new multithreading and multiprocessing libraries and techniques</value><value>static multiprocessing module (smp) (and monkey patching of multiprocessing)</value><value>thread pool control with command line calls of Python ( python -m tbb -p 8)</value><value>Putting it all together</value><value>Examples of using static multiprocessing on a large machine to stop oversubscription</value><value>Example of pseudo-daemon process on 4-core machine by processor pinning</value><value>Thread pool control on a simple NumPy example</value><value>Summary - Best practices for using above methods to control multithreading+multiprocessing</value><value>What needs to be done in the space (frameworks and things that need to be exposed)</value><value>Problems that still exist in the area</value><value>Q&amp;A</value></description><date><value>Friday 27 July</value></date><tags><value>Best Practice</value><value>Multi-Processing</value><value>Scientific Libraries (Numpy/Pandas/SciKit/...)</value><value>CPython</value><value>Multi-Threading</value></tags></item><item><title><value>A Taxonomy of Decorators: A-E</value></title><author><value>Andy Fundinger</value></author><description><value>This talk will briefly go over the various decorator syntaxes before breaking up the common usages of decorators into 5 categories. Effectively, these are design patterns for decorators. The usages to be considered are:</value></description><date><value>Friday 27 July</value></date><tags><value>Software Design</value><value>Development</value><value>Programming</value><value>Python general</value></tags></item><item><title><value>A Python implementation in Rust?</value></title><author><value>Windel Bouwman</value></author><description><value>Rust is a fairly new programming language aimed as a competitor of C.
There are already attempts to write extension modules in rust and load
them into CPython. A whole new approach would be to re-implement the Python
language in rust.</value><value>This is what the rspython project is about: implementing
a python interpreter in rust. Rust already has many features such as vectors,
hashmaps and strings as unicode. There also exist already modules for
regular expressions and dealing with files, so a lot can be re-used in
this area.</value><value>During this talk we will dive into rust and python and the design of
the rspython. We will present some challenges and opportunities. Also
we will show the current state of this project in a demo.</value></description><date><value>Thursday 26 July</value></date><tags><value>Compiler and Interpreters</value><value>Python 3</value></tags></item><item><title><value>A tale of refactoring</value></title><author><value>Emmanuelle Delescolle</value></author><description><value>A couple years ago I was approached by a client to help them improve the performance and maintainability of their code.</value><value>That code was filled with very long functions, some of them with very similar code and the project was really hard to understand as a whole.</value><value>My first reflex was to refactor this code code to make it more understandable. In my opinion, it did fit part of my assignment as understandable code is easier to maintain but it was also very helpful later in letting me make small changes with full knowledge of what was going to be affected by those changes.</value><value>Large methods and functions is something that creeps into our code whether we plan for it or not. This talk is about one way to deal with it.</value><value>This talk will be based on code provided by the GildedRose Refcatoring Kata: https://github.com/emilybache/GildedRose-Refactoring-Kata</value><value>Prerequisites:
- Basic knowledge of Python
- Basic knowledge of OOP</value><value>Goals:
- Demonstrate one approach for refactoring code
- Show how to leverage Python’s dynamic nature to make code simpler</value></description><date><value>Friday 27 July</value></date><tags><value>Case Study</value><value>Programming</value><value>failures/mistakes</value><value>Code Analysis</value><value>Clean Code</value></tags></item><item><title><value>Adventures in compatibility: emulating CPython's C API in PyPy</value></title><author><value>Ronan Lamy</value></author><description><value>PyPy is a fast and compliant implementation of Python. In other words, it’s an interpreter for the Python language that can act as a full replacement for the reference interpreter, CPython. It’s optimised to enable efficient just-in-time compilation of Python code to machine code, and has releases matching versions 2.7, 3.5 and soon(ish) 3.6. The PyPy project also developed cffi as a clean and efficient way of interfacing with C code.</value><value>However, many libraries in the Python ecosystem are implemented as C extensions, which target CPython’s C API. Many others use Cython, which builds C extensions under the hood. Therefore, PyPy needs an emulation layer for the C API. </value><value>This emulation needs to bridge the differences between the implementation languages and the object models of CPython and PyPy. The solution is called cpyext. It’s implemented in a mixture of RPython and C, with most of the API functions and macros implemented in RPython. cpyext exposes PyObjects to the extension code that appear similar to CPython objects (as long as extension writers stay within the fuzzily defined boundaries of the public API) but are merely ‘shadows’ of the real PyPy objects.</value><value>After a brief presentation of PyPy, its goals, and its current statuts and roadmap, this talk will dive into the vexed topic of its handling of C extensions.  By the end of it, the audience should understand the operating principles of cpyext and have a clearer understanding of what happens when you install and run numpy, for instance, on top of PyPy. Some basic familiarity with CPython internals and how C extensions are made will be assumed.</value></description><date><value>Wednesday 25 July</value></date><tags><value>PyPy</value><value>Compiler and Interpreters</value><value>C-Languages</value><value>Scientific Libraries (Numpy/Pandas/SciKit/...)</value></tags></item><item><title><value>A Jupyter Enhancement Proposal Story</value></title><author><value>Raniere Silva</value></author><description><value>Python users should be familiar with the concept of Python Enhancement Proposals (PEPs), the way that the Python language evolves over time. In a similar fashion, the Jupyter project has Jupyter Enhancement Proposals (JEPs). This talk with cover the proposer first-hand experience when submiting JEP 23 - Add Template as Metatada enhancement proposal from it’s beginning, during EuroPython 2017, up to its current status. We will, in addition, present efforts made as part of the OpenDreamKit project to perform Jupyter notebooks conversions using custom metadata, templates, and exporters, in a programmatic way. </value><value>Outline</value><value>0:00 - 0:05 Who are we? We are impostors!
0:05 - 0:10 Our previous experience with Jupyter Notebook. We will talk about the time that Software Carpentry used Jupyter Notebook for their lesson creation and OpenDreamKit Jupyter notebook programmatic notebooks conversion.
0:10 - 0:15 You are not alone. We will talk how the idea for the Jupyter Enhancement Proposals (JEPs) borned at EuroPython 2017 Help Desk
0:15 - 0:20 Writing our first Jupyter Enhancement Proposals. We will cover our steps to create the pull request required by the Jupyter Project.
0:20 - 0:25 What is the current status of the Jupyter Enhancement Proposals? We will cover any progress from the time of this talk proposal submission and the date of it presentation.
0:25 - 0:30 Time for questions </value></description><date><value>Wednesday 25 July</value></date><tags><value>Case Study</value><value>Community</value><value>Jupyter/iPython</value></tags></item><item><title><value>10 years of EuroPython and the Python community</value></title><author><value>Dougal Matthews</value></author><description><value>This talk will take you on a journey from Birmingham to Edinburgh via Florence, Berlin, Bilbao and Rimini. The last 10 years of EuroPython.</value><value>This will be a trip down memory lane where we look at how the language, community and conference have evolved with some personal anecdotes along the way.</value><value>This will be in part a retrospective, where we will look at what we have done well and where we can do better. Finally, we will look at some aspirations and ideas for the coming years.</value></description><date><value>Thursday 26 July</value></date><tags><value>EuroPython</value><value>Community</value></tags></item><item><title><value>White Mars: living far away from any form of life</value></title><author><value>Marco Buttu</value></author><description><value>Concordia Station is a French/Italian facility located inside Antarctica, in a plateau called Dome-C, in the middle of nowhere. A dark and cold place: no Sun from May to August, temperatures around -80 Celsius degress, no life. Here I am living and performing scientific research with other 12 collegues from Italy, France and Austria. We are the most isolated people on Earth, more than the austronauts in the International Space Station. There is no way to move from Concordia until November, and no one can come. It is like to live in another planet, and that is why the European Space Agency is interested in making bio-medical research on us, in order to better understand how the human body behaves in a such extraterrestrial environment. We will introduce our studies, describe this place and our life here, and of course also speak about Python.</value></description><date><value>Wednesday 25 July</value></date><tags><value>Science</value><value>Case Study</value></tags></item><item><title><value>How to Ignore Most Startup Advice and Build a Decent Software Business</value></title><author><value>Ines Montani</value></author><description><value>It’s a great time to be a software developer. Platforms are steadily becoming more mature, useful tools are released almost daily and things that seemed hopelessly futuristic only a few years ago are suddenly commercially viable. Despite this, the software world is awash with bullshit. The success of the largest technology companies has led to a very skewed set of lessons. This narrow focus is amplified by the venture capital industry and the fact that nobody really knows what’s going to happen next.</value><value>The good news is, none of this actually matters. The basics of creating something useful and selling it for money remain the same. In this talk, I’m not going to give you “one weird trick” or tell you to ~* just follow your dreams *~. But I’ll share some of the things we’ve learned from building a successful software company around commercial developer tools and our open-source library spaCy.</value></description><date><value>Thursday 26 July</value></date><tags><value>Best Practice</value><value>Natural Language Processing</value><value>Business</value><value>failures/mistakes</value><value>Use Case</value></tags></item><item><title><value>Die Threads</value></title><author><value>David Beazley</value></author><description><value>In the brave new world of async, threads are now a thing of the past. Or are they not?</value></description><date><value>Wednesday 25 July</value></date><tags><value>Python general</value><value>ASYNC / Concurreny</value><value>Multi-Threading</value></tags></item></items>